{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5deb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabpfn --no-index --find-links=file:///kaggle/input/pip-packages-icr/pip-packages\n",
    "!pip install ipywidgets\n",
    "!mkdir -p /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n",
    "!cp /kaggle/input/pip-packages-icr/pip-packages/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Joblib Module from Scikit Learn\n",
    "import joblib\n",
    "\n",
    "import numpy as np                       # NumPy for numerical computations\n",
    "import pandas as pd                      # Pandas for data manipulation and analysis\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, normalize   # LabelEncoder for encoding categorical variables, normalize for feature scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier   # GradientBoostingClassifier and RandomForestClassifier for classification models\n",
    "from tabpfn import TabPFNClassifier \n",
    "import xgboost   # XGBoost for gradient boosting models\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score   # accuracy_score for evaluating model performance\n",
    "from sklearn.impute import SimpleImputer   # SimpleImputer for handling missing values\n",
    "import imblearn   # imblearn for imbalanced dataset handling\n",
    "from imblearn.over_sampling import RandomOverSampler   # RandomOverSampler for oversampling minority class\n",
    "from imblearn.under_sampling import RandomUnderSampler   # RandomUnderSampler for undersampling majority class\n",
    "import inspect   # inspect for retrieving information about live objects\n",
    "from collections import defaultdict   # defaultdict for creating a dictionary with default values\n",
    "import warnings   # warnings for ignoring warnings during runtime\n",
    "from sklearn.model_selection import KFold as KF\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b863f",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692fb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/icr-identify-age-related-conditions/train.csv')\n",
    "greeks = pd.read_csv('../datasets/icr-identify-age-related-conditions/greeks.csv')\n",
    "test = pd.read_csv('../datasets/icr-identify-age-related-conditions/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 'Id' 'EJ' columns\n",
    "train = train.drop(['Id','EJ'], axis=1)\n",
    "\n",
    "#fill 'median' for missing values\n",
    "columns = train.columns\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy ='median')\n",
    "imputer = imputer.fit(train)\n",
    "train = imputer.transform(train)\n",
    "train = pd.DataFrame(train, columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6afe0",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1badbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "splitter = skf.split(train, train.Class)\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(splitter):\n",
    "    print(f'Getting fold number {fold_idx}')\n",
    "    x_train, y_train = train.iloc[train_idx], greeks.Alpha.iloc[train_idx]\n",
    "    df_train = pd.concat((x_train, y_train), axis=1)\n",
    "    df_val = train.iloc[val_idx]\n",
    "    \n",
    "    #drop column Id & reset index\n",
    "    \n",
    "#     df_train = df_train.drop(['Id'], axis=1)\n",
    "    df_train = df_train.reset_index(drop = True)\n",
    "#     df_val = df_val.drop(['Id'], axis=1)\n",
    "    df_val = df_val.reset_index(drop = True)\n",
    "    \n",
    "    #kfold path\n",
    "    save_dir = f'../datasets/kfold/fold{fold_idx}'\n",
    "    os.makedirs(save_dir, exist_ok = True)\n",
    "    \n",
    "    #saving\n",
    "    df_train.to_csv(os.path.join(save_dir, 'train.csv'), index = False)\n",
    "    df_val.to_csv(os.path.join(save_dir, 'val.csv'), index = False)\n",
    "    \n",
    "    # for testing\n",
    "    save_dir1 = f'../datasets/kfold1/fold{fold_idx}'\n",
    "    os.makedirs(save_dir1, exist_ok = True)\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_val = df_val.reset_index(drop=True)\n",
    "    df_train.to_csv(os.path.join(save_dir1, 'train.csv'))\n",
    "    df_val.to_csv(os.path.join(save_dir1, 'val.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83e862",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665012ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepair_input(df, classi):\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Convert the values in the 'EJ' column of the 'test' dataframe to binary values (0 or 1),\n",
    "    # based on the occurrence of the 'first_category' in the 'train' dataframe\n",
    "#     first_category = df.EJ.unique()[0]\n",
    "#     df.EJ = df.EJ.eq(first_category).astype('int')\n",
    "\n",
    "    df = df.rename(columns={'BD ': 'BD', 'CD ': 'CD', 'CW ': 'CW', 'FD ': 'FD'})\n",
    "    \n",
    "    imputer = SimpleImputer(missing_values = np.nan, strategy ='median')\n",
    "    imputer = imputer.fit(df)\n",
    "    df = imputer.transform(df)\n",
    "    df = pd.DataFrame(df, columns = columns)\n",
    "    \n",
    "    # Create a RandomOverSampler object with a random state of 42\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "    # Resample the 'train_pred_and_time' dataframe and 'greeks.Alpha' series using RandomOverSampler\n",
    "    # The resampled data is assigned to 'train_ros' and 'y_ros' respectively\n",
    "    x_ros, y_ros = ros.fit_resample(df, classi)\n",
    "#     print(y_ros.value_counts())\n",
    "    return x_ros, y_ros\n",
    "\n",
    "def normolized(df):\n",
    "    columns = df.columns\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    model = scaler.fit(df)\n",
    "    scaled_df = model.transform(df)\n",
    "    \n",
    "    scaled_df = pd.DataFrame(scaled_df, columns = columns)\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb6d8e",
   "metadata": {},
   "source": [
    "## Balanced Log Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_log_loss(y_true, y_pred):\n",
    "    #number of true values of 0 & 1\n",
    "    N_0 = np.sum(1 - y_true)\n",
    "    N_1 = np.sum(y_true)\n",
    "    # calculate the weights for each class to balance classes\n",
    "    w_0 = 1 / N_0\n",
    "    w_1 = 1 / N_1\n",
    "    # calculate the predicted probabilities for each class\n",
    "    p_0 = np.clip(y_pred[:, 0], 1e-15, 1 - 1e-15)\n",
    "    p_1 = np.clip(y_pred[:, 1], 1e-15, 1 - 1e-15)\n",
    "    # calculate the summed log loss for each class\n",
    "    log_loss_0 = -np.sum((1 - y_true) * np.log(p_0))\n",
    "    log_loss_1 = -np.sum(y_true * np.log(p_1))\n",
    "    # calculate the weighted summed logarithmic loss\n",
    "    # (factgor of 2 included to give same result as LL with balanced input)\n",
    "    balanced_log_loss = 2*(w_0 * log_loss_0 + w_1 * log_loss_1) / (w_0 + w_1)\n",
    "    # return the average log loss\n",
    "    return balanced_log_loss/(N_0+N_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc5879",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp_true = np.array([0, 0, 1, 0, 1])\n",
    "_tmp_pred = np.array([\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 0]\n",
    "], dtype = np.float32)\n",
    "_tmp_pred = 1.0 - _tmp_pred\n",
    "print(_tmp_pred)\n",
    "\n",
    "\n",
    "balanced_log_loss(_tmp_true, _tmp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec86dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d7cdbf7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1131e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tab = TabPFNClassifier(N_ensemble_configurations=12)\n",
    "type(tab)\n",
    "tabpfn = [TabPFNClassifier(N_ensemble_configurations=12),\n",
    "          TabPFNClassifier(N_ensemble_configurations=24)]\n",
    "\n",
    "type(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a225d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabpfn = TabPFNClassifier(N_ensemble_configurations=12)\n",
    "\n",
    "config1 = [\n",
    "    {'name': 'xgb', 'n_estimators': 100 , 'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.85},\n",
    "    {'name': 'xgb', 'n_estimators': 200 , 'max_depth': 3, 'learning_rate': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.85},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 12},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 24},\n",
    "    {'name': 'randomforest', 'n_estimators': 200, 'max_depth': 3, 'random_state': 42},\n",
    "    {'name': 'randomforest', 'n_estimators': 200, 'max_depth': 3, 'random_state': 42},\n",
    "    {'name': 'gaussiannb'},\n",
    "    {'name': 'multinomialnb'},\n",
    "    {'name': 'kneighbors'},\n",
    "    {'name': 'gradientboosting'}\n",
    "] * 2\n",
    "\n",
    "config2 = [\n",
    "    {'name': 'xgb', 'n_estimators': 100 , 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.85},\n",
    "    {'name': 'xgb', 'n_estimators': 100 , 'max_depth': 5, 'learning_rate': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.85},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 12},\n",
    "    {'name': 'randomforest', 'n_estimators': 100, 'max_depth': 5, 'random_state': 50},\n",
    "    {'name': 'randomforest', 'n_estimators': 100, 'max_depth': 5, 'random_state': 50},\n",
    "    {'name': 'gaussiannb'},\n",
    "    {'name': 'multinomialnb'},\n",
    "    {'name': 'kneighbors'},\n",
    "    {'name': 'gradientboosting'}\n",
    "]\n",
    "\n",
    "config3 = [\n",
    "    {'name': 'xgb', 'n_estimators': 100 , 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 0.9, 'colsample_bytree': 0.85},\n",
    "    {'name': 'randomforest', 'n_estimators': 100, 'max_depth': 7, 'random_state': 30},\n",
    "    {'name': 'randomforest', 'n_estimators': 100, 'max_depth': 7, 'random_state': 30}\n",
    "]\n",
    "\n",
    "config = config1+config2+config3\n",
    "\n",
    "def config_classifiers(config):\n",
    "    CLASSIFIER_CLASSES = {\n",
    "        'xgb': XGBClassifier,\n",
    "        'tabpfn': TabPFNClassifier,\n",
    "        'randomforest': RandomForestClassifier,\n",
    "        'gaussiannb': GaussianNB,\n",
    "        'multinomialnb': MultinomialNB,\n",
    "        'kneighbors': KNeighborsClassifier,\n",
    "        'gradientboosting': GradientBoostingClassifier\n",
    "    }\n",
    "\n",
    "    thismodule = sys.modules[__name__]\n",
    "    classifiers = []\n",
    "    for sub_cfg in config:\n",
    "    #     cls = globals()[sub_cfg['name']]\n",
    "        cls = CLASSIFIER_CLASSES[sub_cfg['name']]\n",
    "        kwargs = {k:v for k, v in sub_cfg.items() if k != 'name'}\n",
    "        classifier = cls(**kwargs)\n",
    "        classifiers.append(classifier)\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble():\n",
    "    def __init__(self):\n",
    "        self.classifiers = config_classifiers(config)\n",
    "        print(self.classifiers)\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        for classifier in self.classifiers:\n",
    "            print(classifier)\n",
    "            if (type(classifier) == type(tabpfn)):\n",
    "                classifier.fit(X, y, overwrite_warning=True)\n",
    "            else :\n",
    "                classifier.fit(X, y)\n",
    "     \n",
    "    def predict_proba(self, x):\n",
    "        # N_models * N_rows * N_classes (#models * 5 * 4)\n",
    "        probabilities = np.stack([classifier.predict_proba(x) for classifier in self.classifiers])\n",
    "        averaged_probabilities = np.mean(probabilities, axis=0) # N_rows * N_classes\n",
    "        class_0_est_instances = averaged_probabilities[:, 0].sum()  # N_rows\n",
    "        others_est_instances = averaged_probabilities[:, 1:].sum()  # N_rows   \n",
    "        # Weighted probabilities based on class imbalance\n",
    "        new_probabilities = averaged_probabilities * np.array([[1/(class_0_est_instances if i==0 else others_est_instances) for i in range(averaged_probabilities.shape[1])]])\n",
    "        ret =  new_probabilities / np.sum(new_probabilities, axis=1, keepdims=1) \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894fee3",
   "metadata": {},
   "source": [
    "# Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9248f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_prob(probs, shape, thres_1, thres_0):\n",
    "    print('TYPE:', probs.shape, type(probs))\n",
    "    \n",
    "    #transfer to probabilitiy of 2 class: 0 & 1\n",
    "    class_0_prob = probs[:, 0]\n",
    "    others_prob = probs[:, 1:].sum(axis=1)\n",
    "    class_0_prob = class_0_prob.reshape((shape, 1))\n",
    "    others_prob = others_prob.reshape((shape, 1))\n",
    "    \n",
    "#     probs = np.concatenate([class_0_prob, others_prob], axis=-1)\n",
    "#     ret = probs.copy()\n",
    "    col_0 = class_0_prob.copy()\n",
    "    col_0[class_0_prob < thres_1] = 0.0\n",
    "    col_0[class_0_prob > thres_0] = 1.0\n",
    "    col_1 = 1.0 - col_0\n",
    "    ret = np.concatenate([col_0, col_1], axis = -1)\n",
    "    print('ret', type(ret))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc21810",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b25cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    splits = 5   # Total number of splits for the inner cross-validation\n",
    "    models = []   # List to store the trained models for each inner fold\n",
    "    thres_lst = []\n",
    "    loss_lst = []\n",
    "    pred_sets = []\n",
    "    true_sets = []\n",
    "\n",
    "    # Loop over the splits of the inner cross-validation using tqdm for progress visualization\n",
    "    for split in range(splits):\n",
    "        model = Ensemble()\n",
    "        print('fold', split)\n",
    "        #loading train & test dataset for each fold\n",
    "        save_dir = f'../datasets/kfold/fold{split}'\n",
    "\n",
    "        # x_train & y_train\n",
    "        df_train = pd.read_csv(os.path.join(save_dir, 'train.csv'))\n",
    "        x_train = df_train.drop(['Class', 'Alpha'], axis=1)\n",
    "        y_train = df_train.Alpha\n",
    "        #labael-encoder\n",
    "        le = LabelEncoder()\n",
    "        y_train = le.fit_transform(y_train)\n",
    "        # pre-processing\n",
    "        x_train, y_train = prepair_input(x_train, y_train)\n",
    "\n",
    "        # x_val & y_val\n",
    "        df_val = pd.read_csv(os.path.join(save_dir, 'val.csv'))\n",
    "        x_val = df_val.drop(['Class'], axis=1)\n",
    "        y_val = df_val.Class\n",
    "        print(y_val.value_counts())\n",
    "\n",
    "        #fitting model\n",
    "        model.fit(x_train, y_train)   # Fit the model on the training data\n",
    "        models.append(model)   # Append the trained model to the list of models\n",
    "        y_pred = model.predict_proba(x_val) # Predict probabilities for the validation set for 4 classes\n",
    "        shape = y_val.size\n",
    "        \n",
    "        for i in range(shape):\n",
    "            pred_sets.append(y_pred[i])\n",
    "            true_sets.append(y_val[i])\n",
    "    \n",
    "    y_pred = np.array(pred_sets)\n",
    "    y_val = pd.Series(true_sets)\n",
    "    \n",
    "    print('Models', models)\n",
    "    \n",
    "    return models, y_pred, y_val   # Return the list of trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc067d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, y_pred, y_val = training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae785998",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5799cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(y_pred, y_val):    \n",
    "    ret = []\n",
    "    #find best threshold\n",
    "    for thres_1 in np.arange(0, 1, 0.01):\n",
    "        for thres_0 in np.arange(thres_1, 1, 0.01):\n",
    "            shape = len(y_val)\n",
    "            #post processing\n",
    "            y_p = calibrate_prob(y_pred, shape, thres_1, thres_0)\n",
    "\n",
    "            #balanced log loss\n",
    "            loss = balanced_log_loss(y_val, y_p)  # Calculate the balanced log loss between the predicted labels and the true labels\n",
    "\n",
    "    #         # checking\n",
    "    #         y_val = y_val.to_frame()\n",
    "    #         y_val.rename(columns = {'Class': 'gt'}, inplace = True)\n",
    "    #         y_val['pred'] = y_p[:, 1]\n",
    "    # #         print(type(y_val['gt']), type(y_val.loc[0, 'gt']), type(y_val['pred']), type(y_val.loc[0, 'pred']))\n",
    "    #         p00 = y_p[:, 1]\n",
    "    #         p00 = p00.flatten()\n",
    "    #         y_val['prob'] = p00\n",
    "    #         display(y_val)\n",
    "            print('>LOSS=%.5f' % loss)\n",
    "            ret.append([thres_1, thres_0, loss])\n",
    "    \n",
    "    ret = sorted(ret, key= lambda x: x[2])\n",
    "    print('best:\\n', ret[:10])\n",
    "    \n",
    "    return ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a06237",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thres = evaluation(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3cb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35cb8e2d",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Joblib Module from Scikit Learn\n",
    "import joblib\n",
    "\n",
    "# Save RL_Model to file in the current working directory\n",
    "save_dir = f'../model'\n",
    "os.makedirs(save_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74337b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(m, 'ensemble.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f761492",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = joblib.load('ensemble.joblib')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Id = test['Id']\n",
    "test = test.drop(['Id', 'EJ'], axis=1)\n",
    "loss_ensembles = list()\n",
    "for ensemble in models:\n",
    "    y_pred = ensemble.predict_proba(test)\n",
    "    shape = test.shape[0]\n",
    "    y_p = calibrate_prob(y_pred, shape, thres)\n",
    "    loss_ensembles.append(y_p)\n",
    "    \n",
    "print(loss_ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ed742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#post processing\n",
    "shape = test.shape[0]\n",
    "\n",
    "y_p = calibrate_prob(y_pred, shape)\n",
    "submission = pd.DataFrame(Id, columns=['Id'])\n",
    "submission[\"class_0\"] = y_p[:, 0]\n",
    "submission[\"class_1\"] = y_p[:, 1]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0330221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5922c314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab65e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('../datasets/icr-identify-age-related-conditions/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26643e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0bdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.read_csv('../datasets/kfold/fold5/val.csv')\n",
    "b = pd.read_csv('../datasets/kfold/fold6/val.csv')\n",
    "a = pd.concat([c, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed81962",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501fbc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'../datasets/sample_test/'\n",
    "os.makedirs(save_dir, exist_ok = True)\n",
    "    \n",
    "#saving\n",
    "a.to_csv(os.path.join(save_dir, 'test.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3db22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #take 'Id' column and drop 'Id', 'EJ' columns\n",
    "# Id = a['Id']\n",
    "test = a.drop(['Class'], axis=1)\n",
    "columns = test.columns\n",
    "\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy ='median')\n",
    "imputer = imputer.fit(test)\n",
    "test = imputer.transform(test)\n",
    "test = pd.DataFrame(test, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e793f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepair_test(df):\n",
    "    #take 'Id' column and drop 'Id', 'EJ' columns\n",
    "    Id = df['Id']\n",
    "    test = df.drop(['Id', 'EJ'], axis=1)\n",
    "    columns = test.columns\n",
    "    \n",
    "    imputer = SimpleImputer(missing_values = np.nan, strategy ='median')\n",
    "    imputer = imputer.fit(test)\n",
    "    test = imputer.transform(test)\n",
    "    test = pd.DataFrame(test, columns = columns)\n",
    "    return Id, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac9f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('../datasets/sample_test/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466317e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Id , test = prepair_test(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eae8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thres[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = test.Class\n",
    "test = test.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8cbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = models[0]\n",
    "\n",
    "y_pred = ensemble.predict_proba(test)\n",
    "print(y_pred)\n",
    "shape = test.shape[0]\n",
    "y_p = calibrate_prob(y_pred, shape, best_thres[0], best_thres[1])\n",
    "loss = balanced_log_loss(y_val, y_p)\n",
    "\n",
    "\n",
    "\n",
    "print(y_p)\n",
    "print(y_val)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ffffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ensembles = list()\n",
    "for ensemble in models:\n",
    "    y_pred = ensemble.predict_proba(test)\n",
    "    print(y_pred)\n",
    "    shape = test.shape[0]\n",
    "    y_p = calibrate_prob(y_pred, shape, best_thres[0], best_thres[1])\n",
    "    loss = balanced_log_loss(y_val, y_p)\n",
    "    loss_ensembles.append(loss)\n",
    "    final_loss = np.mean(loss_ensembles)\n",
    "\n",
    "print('lost_ensembles', loss_ensembles)\n",
    "print('LOSS', final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96bd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = test.shape[0]\n",
    "\n",
    "y_p = calibrate_prob(y_pred, shape, 0.90)\n",
    "y_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf18db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = a.Class\n",
    "loss = balanced_log_loss(y_val, y_p)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec3a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "submission = pd.DataFrame(Id, columns=['Id'])\n",
    "submission[\"class_0\"] = y_p[:, 0]\n",
    "submission[\"class_1\"] = y_p[:, 1]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ecbdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70353514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
