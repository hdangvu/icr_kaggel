{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5deb42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/pip-packages-icr/pip-packages\n",
      "Requirement already satisfied: tabpfn in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (0.1.9)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from tabpfn) (2.29.0)\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from tabpfn) (1.2.2)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from tabpfn) (6.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from tabpfn) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from tabpfn) (1.25.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from requests>=2.23.0->tabpfn) (2023.7.22)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from scikit-learn>=0.24.2->tabpfn) (1.10.1)\n",
      "Requirement already satisfied: filelock in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from torch>=1.9.0->tabpfn) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->tabpfn) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from sympy->torch>=1.9.0->tabpfn) (1.3.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (8.0.7)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipywidgets) (4.0.8)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipywidgets) (8.12.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipywidgets) (3.0.8)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipywidgets) (6.19.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: psutil in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: nest-asyncio in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: appnope in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (23.2.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.9)\n",
      "Requirement already satisfied: packaging in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: decorator in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: stack-data in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: asttokens in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: executing in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.5.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hadangvu/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "mkdir: /opt/conda: Permission denied\n",
      "cp: directory /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff does not exist\n"
     ]
    }
   ],
   "source": [
    "!pip install tabpfn --no-index --find-links=file:///kaggle/input/pip-packages-icr/pip-packages\n",
    "!pip install ipywidgets\n",
    "!mkdir -p /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n",
    "!cp /kaggle/input/pip-packages-icr/pip-packages/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cc071e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# Import Joblib Module from Scikit Learn\n",
    "import joblib\n",
    "\n",
    "import numpy as np                       # NumPy for numerical computations\n",
    "import pandas as pd                      # Pandas for data manipulation and analysis\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, normalize   # LabelEncoder for encoding categorical variables, normalize for feature scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier   # GradientBoostingClassifier and RandomForestClassifier for classification models\n",
    "from tabpfn import TabPFNClassifier \n",
    "import xgboost   # XGBoost for gradient boosting models\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score   # accuracy_score for evaluating model performance\n",
    "from sklearn.impute import SimpleImputer   # SimpleImputer for handling missing values\n",
    "import imblearn   # imblearn for imbalanced dataset handling\n",
    "from imblearn.over_sampling import RandomOverSampler   # RandomOverSampler for oversampling minority class\n",
    "from imblearn.under_sampling import RandomUnderSampler   # RandomUnderSampler for undersampling majority class\n",
    "import inspect   # inspect for retrieving information about live objects\n",
    "from collections import defaultdict   # defaultdict for creating a dictionary with default values\n",
    "import warnings   # warnings for ignoring warnings during runtime\n",
    "from sklearn.model_selection import KFold as KF\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a27b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b863f",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "692fb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/icr-identify-age-related-conditions/train.csv')\n",
    "greeks = pd.read_csv('../datasets/icr-identify-age-related-conditions/greeks.csv')\n",
    "test = pd.read_csv('../datasets/icr-identify-age-related-conditions/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "76b8caac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n"
     ]
    }
   ],
   "source": [
    "#drop 'Id' 'EJ' columns\n",
    "train = train.drop(['Id','EJ'], axis=1)\n",
    "\n",
    "#fill 'median' for missing values\n",
    "columns = train.columns\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy ='median')\n",
    "imputer = imputer.fit(train)\n",
    "train = imputer.transform(train)\n",
    "train = pd.DataFrame(train, columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6afe0",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a1badbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting fold number 0\n",
      "Getting fold number 1\n",
      "Getting fold number 2\n",
      "Getting fold number 3\n",
      "Getting fold number 4\n"
     ]
    }
   ],
   "source": [
    "NUM_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "splitter = skf.split(train, train.Class)\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(splitter):\n",
    "    print(f'Getting fold number {fold_idx}')\n",
    "    x_train, y_train = train.iloc[train_idx], greeks.Alpha.iloc[train_idx]\n",
    "    df_train = pd.concat((x_train, y_train), axis=1)\n",
    "    df_val = train.iloc[val_idx]\n",
    "    \n",
    "    #drop column Id & reset index\n",
    "    \n",
    "#     df_train = df_train.drop(['Id'], axis=1)\n",
    "    df_train = df_train.reset_index(drop = True)\n",
    "#     df_val = df_val.drop(['Id'], axis=1)\n",
    "    df_val = df_val.reset_index(drop = True)\n",
    "    \n",
    "    #kfold path\n",
    "    save_dir = f'../datasets/kfold/fold{fold_idx}'\n",
    "    os.makedirs(save_dir, exist_ok = True)\n",
    "    \n",
    "    #saving\n",
    "    df_train.to_csv(os.path.join(save_dir, 'train.csv'), index = False)\n",
    "    df_val.to_csv(os.path.join(save_dir, 'val.csv'), index = False)\n",
    "    \n",
    "    # for testing\n",
    "    save_dir1 = f'../datasets/kfold1/fold{fold_idx}'\n",
    "    os.makedirs(save_dir1, exist_ok = True)\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_val = df_val.reset_index(drop=True)\n",
    "    df_train.to_csv(os.path.join(save_dir1, 'train.csv'))\n",
    "    df_val.to_csv(os.path.join(save_dir1, 'val.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83e862",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "665012ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepair_test(df):\n",
    "    #take 'Id' column and drop 'Id', 'EJ' columns\n",
    "    Id = df['Id']\n",
    "    test = df.drop(['Id', 'EJ'], axis=1)\n",
    "    columns = test.columns\n",
    "    \n",
    "    imputer = SimpleImputer(missing_values = np.nan, strategy ='median')\n",
    "    imputer = imputer.fit(test)\n",
    "    test = imputer.transform(test)\n",
    "    test = pd.DataFrame(test, columns = columns)\n",
    "    return Id, test\n",
    "\n",
    "def prepair_input(df, classi):\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Convert the values in the 'EJ' column of the 'test' dataframe to binary values (0 or 1),\n",
    "    # based on the occurrence of the 'first_category' in the 'train' dataframe\n",
    "#     first_category = df.EJ.unique()[0]\n",
    "#     df.EJ = df.EJ.eq(first_category).astype('int')\n",
    "\n",
    "    df = df.rename(columns={'BD ': 'BD', 'CD ': 'CD', 'CW ': 'CW', 'FD ': 'FD'})\n",
    "    \n",
    "    imputer = SimpleImputer(missing_values = np.nan, strategy ='median')\n",
    "    imputer = imputer.fit(df)\n",
    "    df = imputer.transform(df)\n",
    "    df = pd.DataFrame(df, columns = columns)\n",
    "    \n",
    "    # Create a RandomOverSampler object with a random state of 42\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "    # Resample the 'train_pred_and_time' dataframe and 'greeks.Alpha' series using RandomOverSampler\n",
    "    # The resampled data is assigned to 'train_ros' and 'y_ros' respectively\n",
    "    x_ros, y_ros = ros.fit_resample(df, classi)\n",
    "#     print(y_ros.value_counts())\n",
    "    return x_ros, y_ros\n",
    "\n",
    "def normolized(df):\n",
    "    columns = df.columns\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    model = scaler.fit(df)\n",
    "    scaled_df = model.transform(df)\n",
    "    \n",
    "    scaled_df = pd.DataFrame(scaled_df, columns = columns)\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb6d8e",
   "metadata": {},
   "source": [
    "## Balanced Log Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3f78b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_log_loss(y_true, y_pred):\n",
    "    #number of true values of 0 & 1\n",
    "    N_0 = np.sum(1 - y_true)\n",
    "    N_1 = np.sum(y_true)\n",
    "    # calculate the weights for each class to balance classes\n",
    "    w_0 = 1 / N_0\n",
    "    w_1 = 1 / N_1\n",
    "    # calculate the predicted probabilities for each class\n",
    "    p_0 = np.clip(y_pred[:, 0], 1e-15, 1 - 1e-15)\n",
    "    p_1 = np.clip(y_pred[:, 1], 1e-15, 1 - 1e-15)\n",
    "    # calculate the summed log loss for each class\n",
    "    log_loss_0 = -np.sum((1 - y_true) * np.log(p_0))\n",
    "    log_loss_1 = -np.sum(y_true * np.log(p_1))\n",
    "    # calculate the weighted summed logarithmic loss\n",
    "    # (factgor of 2 included to give same result as LL with balanced input)\n",
    "    balanced_log_loss = 2*(w_0 * log_loss_0 + w_1 * log_loss_1) / (w_0 + w_1)\n",
    "    # return the average log loss\n",
    "    return balanced_log_loss/(N_0+N_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aafc5879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tmp_true = np.array([0, 0, 1, 0, 1])\n",
    "_tmp_pred = np.array([\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 0]\n",
    "], dtype = np.float32)\n",
    "_tmp_pred = 1.0 - _tmp_pred\n",
    "print(_tmp_pred)\n",
    "\n",
    "\n",
    "balanced_log_loss(_tmp_true, _tmp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec86dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d7cdbf7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "83e1131e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tabpfn.scripts.transformer_prediction_interface.TabPFNClassifier"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab = TabPFNClassifier(N_ensemble_configurations=12)\n",
    "type(tab)\n",
    "tabpfn = [TabPFNClassifier(N_ensemble_configurations=12),\n",
    "          TabPFNClassifier(N_ensemble_configurations=24)]\n",
    "\n",
    "type(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1a225d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    }
   ],
   "source": [
    "tabpfn = TabPFNClassifier(N_ensemble_configurations=12)\n",
    "\n",
    "config1 = [\n",
    "    {'name': 'xgb', 'n_estimators': 100 , 'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.9, 'colsample_bytree': 1, 'seed': 21},\n",
    "    {'name': 'xgb', 'n_estimators': 150 , 'max_depth': 3, 'learning_rate': 0.2, 'subsample': 0.95, 'colsample_bytree': 0.7, 'seed': 36},\n",
    "    {'name': 'xgb', 'n_estimators': 200 , 'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.85, 'seed': 45},\n",
    "    {'name': 'xgb', 'n_estimators': 150 , 'max_depth': 4, 'learning_rate': 0.15, 'subsample': 0.7, 'colsample_bytree': 0.85, 'seed': 65},\n",
    "    {'name': 'xgb', 'n_estimators': 250 , 'max_depth': 2, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.9, 'seed': 26},\n",
    "    {'name': 'xgb', 'n_estimators': 100 , 'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.75, 'colsample_bytree': 1, 'seed': 11},\n",
    "    {'name': 'xgb', 'n_estimators': 250 , 'max_depth': 3, 'learning_rate': 0.2, 'subsample': 0.65, 'colsample_bytree': 0.7, 'seed': 12},\n",
    "    {'name': 'xgb', 'n_estimators': 300 , 'max_depth': 5, 'learning_rate': 0.15, 'subsample': 0.85, 'colsample_bytree': 0.95, 'seed': 13},\n",
    "    {'name': 'xgb', 'n_estimators': 150 , 'max_depth': 4, 'learning_rate': 0.15, 'subsample': 0.7, 'colsample_bytree': 0.85, 'seed': 14},\n",
    "    {'name': 'xgb', 'n_estimators': 250 , 'max_depth': 5, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.9, 'seed': 15},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 12},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 24},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 12},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 24},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 24},\n",
    "    {'name': 'randomforest', 'n_estimators': 200, 'max_depth': 4, 'random_state': 42},\n",
    "    {'name': 'randomforest', 'n_estimators': 250, 'max_depth': 4, 'random_state': 11},\n",
    "    {'name': 'randomforest', 'n_estimators': 200, 'max_depth': 5, 'random_state': 72},\n",
    "    {'name': 'randomforest', 'n_estimators': 150, 'max_depth': 5, 'random_state': 71},\n",
    "    {'name': 'gaussiannb'},\n",
    "    {'name': 'multinomialnb'},\n",
    "    {'name': 'kneighbors', 'n_neighbors': 10, 'algorithm': 'kd_tree', 'leaf_size': 30},\n",
    "    {'name': 'kneighbors', 'n_neighbors': 10, 'algorithm': 'brute', 'leaf_size': 30},\n",
    "    {'name': 'kneighbors', 'n_neighbors': 10, 'algorithm': 'kd_tree', 'leaf_size': 40},\n",
    "    {'name': 'kneighbors', 'n_neighbors': 5, 'algorithm': 'brute', 'leaf_size': 50},\n",
    "    {'name': 'gradientboosting'}\n",
    "]\n",
    "\n",
    "config2 = [\n",
    "    {'name': 'xgb', 'n_estimators': 400 , 'max_depth': 6, 'learning_rate': 0.15, 'subsample': 0.9, 'colsample_bytree': 0.85, 'seed': 0},\n",
    "    {'name': 'xgb', 'n_estimators': 300 , 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.85, 'seed': 3},\n",
    "    {'name': 'xgb', 'n_estimators': 100 , 'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.9, 'colsample_bytree': 1, 'seed': 5},\n",
    "    {'name': 'xgb', 'n_estimators': 150 , 'max_depth': 3, 'learning_rate': 0.2, 'subsample': 0.95, 'colsample_bytree': 0.7, 'seed': 96},\n",
    "    {'name': 'xgb', 'n_estimators': 200 , 'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.85, 'seed': 90},\n",
    "    {'name': 'xgb', 'n_estimators': 150 , 'max_depth': 4, 'learning_rate': 0.15, 'subsample': 0.7, 'colsample_bytree': 0.85, 'seed': 68},\n",
    "    {'name': 'xgb', 'n_estimators': 250 , 'max_depth': 2, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.9, 'seed': 6},\n",
    "    {'name': 'xgb', 'n_estimators': 100 , 'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.75, 'colsample_bytree': 1, 'seed': 99},\n",
    "    {'name': 'xgb', 'n_estimators': 250 , 'max_depth': 3, 'learning_rate': 0.2, 'subsample': 0.65, 'colsample_bytree': 0.7, 'seed': 93},\n",
    "    {'name': 'xgb', 'n_estimators': 300 , 'max_depth': 5, 'learning_rate': 0.15, 'subsample': 0.85, 'colsample_bytree': 0.95, 'seed': 53},\n",
    "    {'name': 'xgb', 'n_estimators': 150 , 'max_depth': 4, 'learning_rate': 0.15, 'subsample': 0.7, 'colsample_bytree': 0.85, 'seed': 94},\n",
    "    {'name': 'xgb', 'n_estimators': 250 , 'max_depth': 5, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.9, 'seed': 75},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 12},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 24},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 12},\n",
    "    {'name': 'randomforest', 'n_estimators': 100, 'max_depth': 5, 'random_state': 5},\n",
    "    {'name': 'randomforest', 'n_estimators': 100, 'max_depth': 5, 'random_state': 50},\n",
    "    {'name': 'gaussiannb'},\n",
    "    {'name': 'multinomialnb'},\n",
    "    {'name': 'kneighbors', 'n_neighbors': 20, 'algorithm': 'ball_tree', 'leaf_size': 30},\n",
    "    {'name': 'gradientboosting'}\n",
    "#     {'name': 'logisticregression', 'max_iter': 150, 'random_state': 53},\n",
    "#     {'name': 'logisticregression', 'max_iter': 200, 'random_state': 64},\n",
    "#     {'name': 'logisticregression', 'max_iter': 50, 'random_state': 23},\n",
    "#     {'name': 'logisticregression', 'max_iter': 400, 'random_state': 59},\n",
    "#     {'name': 'logisticregression', 'max_iter': 300, 'random_state': 2},\n",
    "]\n",
    "\n",
    "config3 = [\n",
    "    {'name': 'xgb', 'n_estimators': 100 , 'max_depth': 7, 'learning_rate': 0.01, 'subsample': 0.8, 'colsample_bytree': 0.85, 'seed': 86},\n",
    "    {'name': 'xgb', 'n_estimators': 500 , 'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.9, 'colsample_bytree': 1, 'seed': 43},\n",
    "    {'name': 'xgb', 'n_estimators': 150 , 'max_depth': 3, 'learning_rate': 0.2, 'subsample': 0.95, 'colsample_bytree': 0.7, 'seed': 56},\n",
    "    {'name': 'xgb', 'n_estimators': 200 , 'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.85, 'seed': 74},\n",
    "    {'name': 'xgb', 'n_estimators': 150 , 'max_depth': 3, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.85, 'seed': 75},\n",
    "    {'name': 'xgb', 'n_estimators': 250 , 'max_depth': 2, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.9, 'seed': 89},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 64},\n",
    "    {'name': 'tabpfn', 'N_ensemble_configurations': 12},\n",
    "    {'name': 'randomforest', 'n_estimators': 200, 'max_depth': 7, 'random_state': 22},\n",
    "    {'name': 'randomforest', 'n_estimators': 500, 'max_depth': 6, 'random_state': 77}\n",
    "]\n",
    "\n",
    "config = config1+config2+config3\n",
    "\n",
    "def config_classifiers(config):\n",
    "    CLASSIFIER_CLASSES = {\n",
    "        'xgb': XGBClassifier,\n",
    "        'tabpfn': TabPFNClassifier,\n",
    "        'randomforest': RandomForestClassifier,\n",
    "        'gaussiannb': GaussianNB,\n",
    "        'multinomialnb': MultinomialNB,\n",
    "        'kneighbors': KNeighborsClassifier,\n",
    "        'gradientboosting': GradientBoostingClassifier,\n",
    "        'logisticregression': LogisticRegression\n",
    "    }\n",
    "\n",
    "    thismodule = sys.modules[__name__]\n",
    "    classifiers = []\n",
    "    for sub_cfg in config:\n",
    "    #     cls = globals()[sub_cfg['name']]\n",
    "        cls = CLASSIFIER_CLASSES[sub_cfg['name']]\n",
    "        kwargs = {k:v for k, v in sub_cfg.items() if k != 'name'}\n",
    "        classifier = cls(**kwargs)\n",
    "        classifiers.append(classifier)\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "107d8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble():\n",
    "    def __init__(self):\n",
    "        self.classifiers = config_classifiers(config)\n",
    "        print(self.classifiers)\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        for classifier in self.classifiers:\n",
    "            print(classifier)\n",
    "            if (type(classifier) == type(tabpfn)):\n",
    "                classifier.fit(X, y, overwrite_warning=True)\n",
    "            else :\n",
    "                classifier.fit(X, y)\n",
    "     \n",
    "    def predict_proba(self, x):\n",
    "        # N_models * N_rows * N_classes (#models * 5 * 4)\n",
    "        probabilities = np.stack([classifier.predict_proba(x) for classifier in self.classifiers])\n",
    "        averaged_probabilities = np.mean(probabilities, axis=0) # N_rows * N_classes\n",
    "        class_0_est_instances = averaged_probabilities[:, 0].sum()  # N_rows\n",
    "        others_est_instances = averaged_probabilities[:, 1:].sum()  # N_rows   \n",
    "        # Weighted probabilities based on class imbalance\n",
    "        new_probabilities = averaged_probabilities * np.array([[1/(class_0_est_instances if i==0 else others_est_instances) for i in range(averaged_probabilities.shape[1])]])\n",
    "        ret =  new_probabilities / np.sum(new_probabilities, axis=1, keepdims=1) \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894fee3",
   "metadata": {},
   "source": [
    "# Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6c9248f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_prob(probs, shape, thres_1, thres_0):\n",
    "    print('TYPE:', probs.shape, type(probs))\n",
    "    \n",
    "    #transfer to probabilitiy of 2 class: 0 & 1\n",
    "    class_0_prob = probs[:, 0]\n",
    "    others_prob = probs[:, 1:].sum(axis=1)\n",
    "    class_0_prob = class_0_prob.reshape((shape, 1))\n",
    "    others_prob = others_prob.reshape((shape, 1))\n",
    "    \n",
    "#     probs = np.concatenate([class_0_prob, others_prob], axis=-1)\n",
    "#     ret = probs.copy()\n",
    "    col_0 = class_0_prob.copy()\n",
    "    col_0[class_0_prob < thres_1] = 0.0\n",
    "    col_0[class_0_prob > thres_0] = 1.0\n",
    "    col_1 = 1.0 - col_0\n",
    "    ret = np.concatenate([col_0, col_1], axis = -1)\n",
    "    print('ret', type(ret))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc21810",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "33b25cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    splits = 5   # Total number of splits for the inner cross-validation\n",
    "    models = []   # List to store the trained models for each inner fold\n",
    "    thres_lst = []\n",
    "    loss_lst = []\n",
    "    pred_sets = []\n",
    "    true_sets = []\n",
    "\n",
    "    # Loop over the splits of the inner cross-validation using tqdm for progress visualization\n",
    "    for split in range(splits):\n",
    "        model = Ensemble()\n",
    "        print('fold', split)\n",
    "        #loading train & test dataset for each fold\n",
    "        save_dir = f'../datasets/kfold/fold{split}'\n",
    "\n",
    "        # x_train & y_train\n",
    "        df_train = pd.read_csv(os.path.join(save_dir, 'train.csv'))\n",
    "        x_train = df_train.drop(['Class', 'Alpha'], axis=1)\n",
    "        y_train = df_train.Alpha\n",
    "        #labael-encoder\n",
    "        le = LabelEncoder()\n",
    "        y_train = le.fit_transform(y_train)\n",
    "        # pre-processing\n",
    "        x_train, y_train = prepair_input(x_train, y_train)\n",
    "\n",
    "        # x_val & y_val\n",
    "        df_val = pd.read_csv(os.path.join(save_dir, 'val.csv'))\n",
    "        x_val = df_val.drop(['Class'], axis=1)\n",
    "        y_val = df_val.Class\n",
    "        print(y_val.value_counts())\n",
    "\n",
    "        #fitting model\n",
    "        model.fit(x_train, y_train)   # Fit the model on the training data\n",
    "        models.append(model)   # Append the trained model to the list of models\n",
    "        y_pred = model.predict_proba(x_val) # Predict probabilities for the validation set for 4 classes\n",
    "        shape = y_val.size\n",
    "        \n",
    "        for i in range(shape):\n",
    "            pred_sets.append(y_pred[i])\n",
    "            true_sets.append(y_val[i])\n",
    "    \n",
    "    y_pred = np.array(pred_sets)\n",
    "    y_val = pd.Series(true_sets)\n",
    "    \n",
    "    print('Models', models)\n",
    "    \n",
    "    return models, y_pred, y_val   # Return the list of trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bc067d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "[XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=21, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=36, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=45, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=65, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=26, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=11, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=12, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=13, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=14, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=15, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=24), RandomForestClassifier(max_depth=4, n_estimators=200, random_state=42), RandomForestClassifier(max_depth=4, n_estimators=250, random_state=11), RandomForestClassifier(max_depth=5, n_estimators=200, random_state=72), RandomForestClassifier(max_depth=5, n_estimators=150, random_state=71), GaussianNB(), MultinomialNB(), KNeighborsClassifier(algorithm='kd_tree', n_neighbors=10), KNeighborsClassifier(algorithm='brute', n_neighbors=10), KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_neighbors=10), KNeighborsClassifier(algorithm='brute', leaf_size=50), GradientBoostingClassifier(), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=400, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=0, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=3, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=5, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=96, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=90, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=68, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=6, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=99, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=93, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=53, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=94, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=12), RandomForestClassifier(max_depth=5, random_state=5), RandomForestClassifier(max_depth=5, random_state=50), GaussianNB(), MultinomialNB(), KNeighborsClassifier(algorithm='ball_tree', n_neighbors=20), GradientBoostingClassifier(), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.01, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=86, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=43, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=56, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=74, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=89, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=64), TabPFNClassifier(N_ensemble_configurations=12), RandomForestClassifier(max_depth=7, n_estimators=200, random_state=22), RandomForestClassifier(max_depth=6, n_estimators=500, random_state=77)]\n",
      "fold 0\n",
      "0.0    102\n",
      "1.0     22\n",
      "Name: Class, dtype: int64\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=21, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=36, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=45, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=65, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=26, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=11, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=12, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=13, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=14, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=15, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "RandomForestClassifier(max_depth=4, n_estimators=200, random_state=42)\n",
      "RandomForestClassifier(max_depth=4, n_estimators=250, random_state=11)\n",
      "RandomForestClassifier(max_depth=5, n_estimators=200, random_state=72)\n",
      "RandomForestClassifier(max_depth=5, n_estimators=150, random_state=71)\n",
      "GaussianNB()\n",
      "MultinomialNB()\n",
      "KNeighborsClassifier(algorithm='kd_tree', n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='brute', n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='brute', leaf_size=50)\n",
      "GradientBoostingClassifier()\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=400, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=0, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=3, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=5, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=96, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=90, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=68, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=6, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=99, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=93, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=53, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=94, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "RandomForestClassifier(max_depth=5, random_state=5)\n",
      "RandomForestClassifier(max_depth=5, random_state=50)\n",
      "GaussianNB()\n",
      "MultinomialNB()\n",
      "KNeighborsClassifier(algorithm='ball_tree', n_neighbors=20)\n",
      "GradientBoostingClassifier()\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.01, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=86, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=43, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=56, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=74, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=89, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:27:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=64)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "RandomForestClassifier(max_depth=7, n_estimators=200, random_state=22)\n",
      "RandomForestClassifier(max_depth=6, n_estimators=500, random_state=77)\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "[XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=21, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=36, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=45, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=65, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=26, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=11, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=12, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=13, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=14, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=15, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=24), RandomForestClassifier(max_depth=4, n_estimators=200, random_state=42), RandomForestClassifier(max_depth=4, n_estimators=250, random_state=11), RandomForestClassifier(max_depth=5, n_estimators=200, random_state=72), RandomForestClassifier(max_depth=5, n_estimators=150, random_state=71), GaussianNB(), MultinomialNB(), KNeighborsClassifier(algorithm='kd_tree', n_neighbors=10), KNeighborsClassifier(algorithm='brute', n_neighbors=10), KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_neighbors=10), KNeighborsClassifier(algorithm='brute', leaf_size=50), GradientBoostingClassifier(), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=400, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=0, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=3, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=5, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=96, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=90, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=68, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=6, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=99, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=93, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=53, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=94, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=12), RandomForestClassifier(max_depth=5, random_state=5), RandomForestClassifier(max_depth=5, random_state=50), GaussianNB(), MultinomialNB(), KNeighborsClassifier(algorithm='ball_tree', n_neighbors=20), GradientBoostingClassifier(), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.01, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=86, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=43, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=56, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=74, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=89, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=64), TabPFNClassifier(N_ensemble_configurations=12), RandomForestClassifier(max_depth=7, n_estimators=200, random_state=22), RandomForestClassifier(max_depth=6, n_estimators=500, random_state=77)]\n",
      "fold 1\n",
      "0.0    102\n",
      "1.0     22\n",
      "Name: Class, dtype: int64\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=21, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=36, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=45, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=65, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=26, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=11, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=12, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=13, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=14, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=15, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "RandomForestClassifier(max_depth=4, n_estimators=200, random_state=42)\n",
      "RandomForestClassifier(max_depth=4, n_estimators=250, random_state=11)\n",
      "RandomForestClassifier(max_depth=5, n_estimators=200, random_state=72)\n",
      "RandomForestClassifier(max_depth=5, n_estimators=150, random_state=71)\n",
      "GaussianNB()\n",
      "MultinomialNB()\n",
      "KNeighborsClassifier(algorithm='kd_tree', n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='brute', n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='brute', leaf_size=50)\n",
      "GradientBoostingClassifier()\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=400, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=0, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=3, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=5, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=96, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=90, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=68, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=6, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=99, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=93, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=53, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=94, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "RandomForestClassifier(max_depth=5, random_state=5)\n",
      "RandomForestClassifier(max_depth=5, random_state=50)\n",
      "GaussianNB()\n",
      "MultinomialNB()\n",
      "KNeighborsClassifier(algorithm='ball_tree', n_neighbors=20)\n",
      "GradientBoostingClassifier()\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.01, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=86, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=43, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=56, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=74, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=89, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:29:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=64)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "RandomForestClassifier(max_depth=7, n_estimators=200, random_state=22)\n",
      "RandomForestClassifier(max_depth=6, n_estimators=500, random_state=77)\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "[XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=21, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=36, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=45, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=65, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=26, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=11, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=12, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=13, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=14, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=15, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=24), RandomForestClassifier(max_depth=4, n_estimators=200, random_state=42), RandomForestClassifier(max_depth=4, n_estimators=250, random_state=11), RandomForestClassifier(max_depth=5, n_estimators=200, random_state=72), RandomForestClassifier(max_depth=5, n_estimators=150, random_state=71), GaussianNB(), MultinomialNB(), KNeighborsClassifier(algorithm='kd_tree', n_neighbors=10), KNeighborsClassifier(algorithm='brute', n_neighbors=10), KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_neighbors=10), KNeighborsClassifier(algorithm='brute', leaf_size=50), GradientBoostingClassifier(), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=400, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=0, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=3, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=5, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=96, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=90, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=68, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=6, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=99, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=93, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=53, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=94, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=12), RandomForestClassifier(max_depth=5, random_state=5), RandomForestClassifier(max_depth=5, random_state=50), GaussianNB(), MultinomialNB(), KNeighborsClassifier(algorithm='ball_tree', n_neighbors=20), GradientBoostingClassifier(), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.01, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=86, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=43, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=56, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=74, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=89, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=64), TabPFNClassifier(N_ensemble_configurations=12), RandomForestClassifier(max_depth=7, n_estimators=200, random_state=22), RandomForestClassifier(max_depth=6, n_estimators=500, random_state=77)]\n",
      "fold 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    101\n",
      "1.0     22\n",
      "Name: Class, dtype: int64\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=21, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=36, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=45, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=65, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=26, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=11, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=12, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=13, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=14, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=15, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "RandomForestClassifier(max_depth=4, n_estimators=200, random_state=42)\n",
      "RandomForestClassifier(max_depth=4, n_estimators=250, random_state=11)\n",
      "RandomForestClassifier(max_depth=5, n_estimators=200, random_state=72)\n",
      "RandomForestClassifier(max_depth=5, n_estimators=150, random_state=71)\n",
      "GaussianNB()\n",
      "MultinomialNB()\n",
      "KNeighborsClassifier(algorithm='kd_tree', n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='brute', n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='brute', leaf_size=50)\n",
      "GradientBoostingClassifier()\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=400, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=0, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=3, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=5, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=96, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=90, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=68, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=6, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=99, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=93, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=53, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=94, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "RandomForestClassifier(max_depth=5, random_state=5)\n",
      "RandomForestClassifier(max_depth=5, random_state=50)\n",
      "GaussianNB()\n",
      "MultinomialNB()\n",
      "KNeighborsClassifier(algorithm='ball_tree', n_neighbors=20)\n",
      "GradientBoostingClassifier()\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.01, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=86, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=43, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=56, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=74, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:31:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=89, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:32:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=64)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "RandomForestClassifier(max_depth=7, n_estimators=200, random_state=22)\n",
      "RandomForestClassifier(max_depth=6, n_estimators=500, random_state=77)\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "[XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=21, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=36, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=45, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=65, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=26, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=11, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=12, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=13, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=14, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=15, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=24), RandomForestClassifier(max_depth=4, n_estimators=200, random_state=42), RandomForestClassifier(max_depth=4, n_estimators=250, random_state=11), RandomForestClassifier(max_depth=5, n_estimators=200, random_state=72), RandomForestClassifier(max_depth=5, n_estimators=150, random_state=71), GaussianNB(), MultinomialNB(), KNeighborsClassifier(algorithm='kd_tree', n_neighbors=10), KNeighborsClassifier(algorithm='brute', n_neighbors=10), KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_neighbors=10), KNeighborsClassifier(algorithm='brute', leaf_size=50), GradientBoostingClassifier(), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=400, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=0, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=3, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=5, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=96, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=90, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=68, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=6, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=99, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=93, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=53, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=94, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=12), RandomForestClassifier(max_depth=5, random_state=5), RandomForestClassifier(max_depth=5, random_state=50), GaussianNB(), MultinomialNB(), KNeighborsClassifier(algorithm='ball_tree', n_neighbors=20), GradientBoostingClassifier(), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.01, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=86, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=43, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=56, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=74, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=89, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=64), TabPFNClassifier(N_ensemble_configurations=12), RandomForestClassifier(max_depth=7, n_estimators=200, random_state=22), RandomForestClassifier(max_depth=6, n_estimators=500, random_state=77)]\n",
      "fold 3\n",
      "0.0    102\n",
      "1.0     21\n",
      "Name: Class, dtype: int64\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=21, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=36, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=45, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=65, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=26, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=11, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=12, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=13, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=14, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=15, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "RandomForestClassifier(max_depth=4, n_estimators=200, random_state=42)\n",
      "RandomForestClassifier(max_depth=4, n_estimators=250, random_state=11)\n",
      "RandomForestClassifier(max_depth=5, n_estimators=200, random_state=72)\n",
      "RandomForestClassifier(max_depth=5, n_estimators=150, random_state=71)\n",
      "GaussianNB()\n",
      "MultinomialNB()\n",
      "KNeighborsClassifier(algorithm='kd_tree', n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='brute', n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='brute', leaf_size=50)\n",
      "GradientBoostingClassifier()\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=400, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=0, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=3, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=5, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=96, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=90, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:33:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=68, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=6, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=99, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=93, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=53, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=94, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "RandomForestClassifier(max_depth=5, random_state=5)\n",
      "RandomForestClassifier(max_depth=5, random_state=50)\n",
      "GaussianNB()\n",
      "MultinomialNB()\n",
      "KNeighborsClassifier(algorithm='ball_tree', n_neighbors=20)\n",
      "GradientBoostingClassifier()\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.01, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=86, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=43, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=56, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=74, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=89, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:34:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=64)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "RandomForestClassifier(max_depth=7, n_estimators=200, random_state=22)\n",
      "RandomForestClassifier(max_depth=6, n_estimators=500, random_state=77)\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "[XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=21, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=36, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=45, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=65, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=26, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=11, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=12, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=13, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=14, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=15, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=24), RandomForestClassifier(max_depth=4, n_estimators=200, random_state=42), RandomForestClassifier(max_depth=4, n_estimators=250, random_state=11), RandomForestClassifier(max_depth=5, n_estimators=200, random_state=72), RandomForestClassifier(max_depth=5, n_estimators=150, random_state=71), GaussianNB(), MultinomialNB(), KNeighborsClassifier(algorithm='kd_tree', n_neighbors=10), KNeighborsClassifier(algorithm='brute', n_neighbors=10), KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_neighbors=10), KNeighborsClassifier(algorithm='brute', leaf_size=50), GradientBoostingClassifier(), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=400, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=0, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=3, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=5, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=96, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=90, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=68, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=6, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=99, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=93, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=53, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=94, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=12), TabPFNClassifier(N_ensemble_configurations=24), TabPFNClassifier(N_ensemble_configurations=12), RandomForestClassifier(max_depth=5, random_state=5), RandomForestClassifier(max_depth=5, random_state=50), GaussianNB(), MultinomialNB(), KNeighborsClassifier(algorithm='ball_tree', n_neighbors=20), GradientBoostingClassifier(), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.01, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=86, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=43, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=56, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=74, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=89, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None), TabPFNClassifier(N_ensemble_configurations=64), TabPFNClassifier(N_ensemble_configurations=12), RandomForestClassifier(max_depth=7, n_estimators=200, random_state=22), RandomForestClassifier(max_depth=6, n_estimators=500, random_state=77)]\n",
      "fold 4\n",
      "0.0    102\n",
      "1.0     21\n",
      "Name: Class, dtype: int64\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=21, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=36, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=45, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=65, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=26, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=11, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=12, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=13, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=14, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=15, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "RandomForestClassifier(max_depth=4, n_estimators=200, random_state=42)\n",
      "RandomForestClassifier(max_depth=4, n_estimators=250, random_state=11)\n",
      "RandomForestClassifier(max_depth=5, n_estimators=200, random_state=72)\n",
      "RandomForestClassifier(max_depth=5, n_estimators=150, random_state=71)\n",
      "GaussianNB()\n",
      "MultinomialNB()\n",
      "KNeighborsClassifier(algorithm='kd_tree', n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='brute', n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_neighbors=10)\n",
      "KNeighborsClassifier(algorithm='brute', leaf_size=50)\n",
      "GradientBoostingClassifier()\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=400, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=0, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=6,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=3, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=5, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=96, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=90, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=68, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=6, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=99, subsample=0.75,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=93, subsample=0.65,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.95,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=300, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=53, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.15, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=94, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "TabPFNClassifier(N_ensemble_configurations=24)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "RandomForestClassifier(max_depth=5, random_state=5)\n",
      "RandomForestClassifier(max_depth=5, random_state=50)\n",
      "GaussianNB()\n",
      "MultinomialNB()\n",
      "KNeighborsClassifier(algorithm='ball_tree', n_neighbors=20)\n",
      "GradientBoostingClassifier()\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.01, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=86, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=1,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_delta_step=None, max_depth=5,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=43, subsample=0.9,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.7,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.2, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=56, subsample=0.95,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=4,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=74, subsample=0.85,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.85,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=3,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=150, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=75, subsample=0.7,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.9,\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.05, max_delta_step=None, max_depth=2,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=250, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, seed=89, subsample=0.8,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None)\n",
      "[00:36:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabPFNClassifier(N_ensemble_configurations=64)\n",
      "TabPFNClassifier(N_ensemble_configurations=12)\n",
      "RandomForestClassifier(max_depth=7, n_estimators=200, random_state=22)\n",
      "RandomForestClassifier(max_depth=6, n_estimators=500, random_state=77)\n",
      "Models [<__main__.Ensemble object at 0x2cd132f50>, <__main__.Ensemble object at 0x145f19750>, <__main__.Ensemble object at 0x288213e80>, <__main__.Ensemble object at 0x28ceff160>, <__main__.Ensemble object at 0x28c520a00>]\n"
     ]
    }
   ],
   "source": [
    "models, y_pred, y_val = training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae785998",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2b5799cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(y_pred, y_val):    \n",
    "    ret = []\n",
    "    #find best threshold\n",
    "    for thres_1 in np.arange(0, 1, 0.01):\n",
    "        for thres_0 in np.arange(thres_1, 1, 0.01):\n",
    "            shape = len(y_val)\n",
    "            #post processing\n",
    "            y_p = calibrate_prob(y_pred, shape, thres_1, thres_0)\n",
    "\n",
    "            #balanced log loss\n",
    "            loss = balanced_log_loss(y_val, y_p)  # Calculate the balanced log loss between the predicted labels and the true labels\n",
    "\n",
    "    #         # checking\n",
    "    #         y_val = y_val.to_frame()\n",
    "    #         y_val.rename(columns = {'Class': 'gt'}, inplace = True)\n",
    "    #         y_val['pred'] = y_p[:, 1]\n",
    "    # #         print(type(y_val['gt']), type(y_val.loc[0, 'gt']), type(y_val['pred']), type(y_val.loc[0, 'pred']))\n",
    "    #         p00 = y_p[:, 1]\n",
    "    #         p00 = p00.flatten()\n",
    "    #         y_val['prob'] = p00\n",
    "    #         display(y_val)\n",
    "            print('>LOSS=%.5f' % loss)\n",
    "            ret.append([thres_1, thres_0, loss])\n",
    "    \n",
    "    ret = sorted(ret, key= lambda x: x[2])\n",
    "    print('best:\\n', ret[:10])\n",
    "    \n",
    "    return ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "86a06237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.69788\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.32862\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.31337\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.75974\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.74519\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.09974\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.54826\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.08890\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.63106\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.54027\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.07996\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.71182\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.25181\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.25294\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06905\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97927\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98132\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98417\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.89420\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.71164\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.71164\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.62245\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.34846\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.25924\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89308\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89454\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80450\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80519\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80587\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71519\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62388\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62449\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44261\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35142\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35142\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35142\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35304\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35356\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26360\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26410\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26508\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26555\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26693\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26737\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26825\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26910\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08871\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08950\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00054\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00166\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00240\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00276\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00413\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00614\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00711\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00961\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83298\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74659\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75002\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75553\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75767\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76652\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68179\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59650\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.50984\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51478\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51829\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52029\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52410\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17247\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17563\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17800\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17937\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18126\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18346\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18478\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18624\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18772\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18865\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18989\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19051\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19116\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19169\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19218\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19250\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19296\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19349\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19357\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19357\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19357\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19357\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19357\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19357\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.69781\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.32855\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.31331\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.75967\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.74513\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.09967\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.54820\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.08883\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.63100\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.54021\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.07989\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.71175\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.25175\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.25287\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06898\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97920\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98125\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98223\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98223\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98410\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.89413\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.71157\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.71157\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.62238\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.34839\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.25918\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89301\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89447\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80443\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80513\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80581\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71512\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62442\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44254\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35135\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35135\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35135\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35297\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35350\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26354\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26403\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26501\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26548\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26686\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26731\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26819\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26903\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08864\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08943\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00047\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00159\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00233\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00269\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00406\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00607\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00704\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00954\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83291\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74652\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74995\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75546\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75760\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76172\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76645\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68172\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59643\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.50977\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51472\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51822\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52022\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52403\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25981\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17240\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17556\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17794\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17931\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18120\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18339\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18618\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18722\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18766\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18858\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18982\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19044\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19109\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19163\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19211\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19243\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19289\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19325\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19343\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19350\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19350\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19350\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19350\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19350\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19350\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.32837\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.31313\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.75949\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.74495\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.09949\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.54802\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.08865\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.63082\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.54003\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.07971\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.71157\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.25157\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.25269\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06880\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97902\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98107\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98205\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98205\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98392\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.89395\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.71139\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.71139\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.62220\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.34821\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.25899\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89283\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89429\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80425\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80495\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80562\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71494\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62363\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62424\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35117\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35117\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35117\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35279\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35331\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26336\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26385\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26483\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26530\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26668\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26713\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26800\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26885\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08846\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08925\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00029\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00141\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00215\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00251\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00388\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00589\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00686\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00936\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83273\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74634\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74977\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75528\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75742\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76154\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76627\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68154\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59625\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.50959\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51454\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51804\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52004\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52385\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25963\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17538\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17776\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17913\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18102\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18321\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18453\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18600\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18704\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18747\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18840\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18964\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19026\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19091\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19145\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19193\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19225\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19271\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19307\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19325\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.31241\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.75878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.74423\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.09877\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.54730\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.08793\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.63010\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.53931\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.07899\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.71085\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.25085\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.25197\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06808\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97830\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98035\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98133\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98133\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98320\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.89323\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.71067\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.71067\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.62149\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.34750\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.25828\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89211\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89358\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80353\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80423\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80491\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71422\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62291\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62352\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44164\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35045\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35045\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35045\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35208\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35260\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26264\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26313\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26411\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26458\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26596\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26641\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26813\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08774\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08853\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99957\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00070\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00143\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00179\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00316\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00517\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00614\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00864\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83201\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74563\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74905\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75456\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75670\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76082\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76556\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68082\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59553\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.50887\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51382\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51733\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51932\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52314\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25891\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17150\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17466\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17704\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17841\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18030\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18249\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18528\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18632\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18676\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18768\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18892\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18955\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19020\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19073\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19121\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19153\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19199\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19235\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19253\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19260\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19260\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19260\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19260\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19260\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19260\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.75825\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.74370\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.09825\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.54677\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.08741\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.62957\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.53878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.07847\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.71033\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.25032\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.25144\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06755\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97777\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97982\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98080\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98080\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98268\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.89270\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.71014\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.71014\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.62096\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.34697\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.25775\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89159\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89305\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80301\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80370\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80438\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71369\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62238\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62300\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44112\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34992\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34992\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34992\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35155\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35207\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26211\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26260\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26359\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26406\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26543\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26588\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26676\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26761\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08722\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08800\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99904\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00017\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00091\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00126\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00263\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00464\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00562\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00812\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83149\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74510\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74853\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75403\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75618\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76029\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76503\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68029\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59501\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.50835\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51329\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51680\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51879\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52261\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25838\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17097\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17651\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17788\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17977\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18197\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18328\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18475\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18579\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18623\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18716\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18839\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18902\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18967\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19020\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19069\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19101\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19147\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19182\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19200\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19208\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19208\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19208\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19208\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19208\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19208\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.74229\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.09683\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.54536\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.08599\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.62816\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.53737\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.07705\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.70891\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.24891\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.25003\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06614\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97636\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97841\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97939\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97939\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98126\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.89129\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.70873\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.70873\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.61954\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.34555\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.25633\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89017\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89163\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80159\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80229\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80297\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71228\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62097\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62158\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43970\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34851\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34851\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34851\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35013\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35065\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26070\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26119\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26217\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26264\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26402\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26447\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26534\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26619\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08580\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08659\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99763\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99875\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99949\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99985\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00122\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00323\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00420\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00670\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83007\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74368\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74711\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75262\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75476\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75888\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76361\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67888\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59359\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.50693\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51188\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51538\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51738\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52119\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25697\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.16956\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17272\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17510\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17647\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17836\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18055\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18187\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18334\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18438\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18482\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18574\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18698\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18760\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18825\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18879\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18927\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18959\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19005\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19041\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19059\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19066\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19066\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19066\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19066\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19066\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19066\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.09577\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.54429\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.08493\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.62709\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.53630\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.07599\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.70785\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.24784\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.24896\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06508\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97529\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97734\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97833\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97833\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98020\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.89023\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.70766\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.70766\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.61848\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.34449\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.25527\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88911\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89057\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80053\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80122\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80190\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71122\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61990\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62052\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43864\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34745\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34745\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34745\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34959\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25963\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26013\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26111\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26158\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26296\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26340\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26428\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26513\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08474\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08553\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99656\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99769\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99843\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00016\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00217\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00314\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00564\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82901\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74262\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74605\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75156\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75370\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75781\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76255\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67782\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59253\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.50587\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51081\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51432\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51631\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52013\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.16850\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17166\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17403\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17540\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.17949\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18080\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18227\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18375\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18468\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18591\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18654\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18719\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18772\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18821\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18853\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18899\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18934\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18952\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18960\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18960\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18960\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18960\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18960\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18960\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.56120\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.10184\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.64400\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.55321\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.09290\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.72476\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.26475\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.26587\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.08199\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.99221\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.99425\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.99524\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.99524\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.99711\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.90714\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.72457\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.72457\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.63539\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.36140\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.27218\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90602\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90748\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81744\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81813\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81881\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.72813\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63681\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63743\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45555\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36436\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36436\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36436\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36598\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36650\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27654\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27704\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27802\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27849\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28031\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28119\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28204\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10165\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10244\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01347\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01460\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01534\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01570\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01707\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01908\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02005\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02255\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84592\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75953\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76296\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76847\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.77061\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.77472\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.77946\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69473\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60944\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52278\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52772\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.53123\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.53323\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.53704\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27281\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18541\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.18857\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19094\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19231\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19420\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19640\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19772\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.19918\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20023\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20066\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20159\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20282\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20345\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20410\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20463\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20512\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20544\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20625\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20643\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20651\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20651\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20651\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20651\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20651\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20651\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.11900\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.66116\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.57037\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.11006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.74192\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.28192\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.28304\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.09915\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.00937\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.01142\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.01240\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.01240\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.01427\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.92430\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.74174\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.74174\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.65255\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.37856\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.28934\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92318\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92464\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83460\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83529\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83597\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74529\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65398\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65459\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.47271\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.38152\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.38152\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.38152\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.38314\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.38366\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29370\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29420\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29518\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29565\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29703\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29747\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29835\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29920\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.11881\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.11960\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03064\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03176\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03250\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03286\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03423\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03624\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03721\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03971\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86308\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.77669\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.78012\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.78563\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.78777\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.79189\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.79662\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71189\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.62660\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.53994\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54488\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54839\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.55039\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.55420\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28998\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20257\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20573\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20810\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.20948\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LOSS=0.21136\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.21356\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.21488\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.21635\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.21739\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.21782\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.21875\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.21999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22061\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22126\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22179\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22228\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22260\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22306\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22342\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22360\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22367\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22367\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22367\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22367\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22367\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.22367\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.69639\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.60560\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.14529\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.77715\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.31714\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.31827\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.13438\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.04460\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.04665\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.04763\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.04763\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.04950\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.95953\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.77697\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.77697\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.68778\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.41379\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.32457\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.95841\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.95987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.86983\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87052\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87120\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.78052\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68921\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68982\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50794\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41675\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41675\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41675\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41837\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41889\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32893\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32943\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33041\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33088\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33226\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33270\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33358\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33443\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.15404\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.15483\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06587\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06699\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06809\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06946\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07147\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07244\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07494\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89831\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81192\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81535\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82086\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82300\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82712\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83185\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74712\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.66183\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.57517\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58011\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58362\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58562\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58943\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32521\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.23780\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.24096\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.24333\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.24471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.24659\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.24879\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25011\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25157\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25262\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25305\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25398\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25522\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25584\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25702\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25751\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25783\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25829\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25865\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25882\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25890\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25890\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25890\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25890\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25890\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25890\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.62363\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.16331\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.79517\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.33517\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.33629\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.15240\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06262\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06565\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06565\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06753\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97755\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.79499\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.79499\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.70581\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43182\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.34260\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97644\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97790\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88785\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88855\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88923\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79854\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70723\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70784\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52597\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43477\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43477\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43477\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43640\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43692\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34696\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34745\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34843\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34891\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35028\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35073\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35161\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35246\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17206\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17285\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08389\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08502\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08576\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08611\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08748\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08949\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09047\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09297\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.91633\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82995\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83337\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83888\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84103\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84514\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84988\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76514\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67986\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59320\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59814\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60165\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60364\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60746\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.34323\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25582\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25898\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26136\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26273\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26462\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26681\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26813\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26960\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27064\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27108\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27200\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27324\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27387\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27452\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27505\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27553\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27586\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27632\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27667\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27685\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27692\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27692\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27692\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27692\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27692\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27692\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.16183\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.79369\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.33369\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.33481\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.15092\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06114\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06319\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06417\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06417\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06604\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97607\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.79351\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.79351\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.70432\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43033\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.34111\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97495\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97641\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88637\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88707\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88774\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79706\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70575\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70636\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52448\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43329\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43329\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43329\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43491\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43543\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34548\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34597\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34695\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34742\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34880\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34924\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35012\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35097\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17058\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17137\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08241\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08353\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08427\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08463\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08600\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08801\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08898\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09148\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.91485\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82846\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83189\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83740\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83954\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84366\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84839\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76366\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67837\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59171\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59665\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60016\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60216\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60597\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.34175\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25434\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25750\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25988\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26125\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26314\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26533\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26665\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26812\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26916\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26959\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27052\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27176\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27238\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27303\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27356\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27405\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27437\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27483\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27519\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27537\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27544\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27544\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27544\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27544\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27544\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27544\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.79239\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.33239\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.33351\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.14962\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.05984\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06189\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06287\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06287\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06474\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97477\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.79221\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.79221\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.70302\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.42903\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.33981\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97365\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97511\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88507\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88577\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88644\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79576\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70445\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70506\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52318\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43199\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43199\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43199\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43361\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43413\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34417\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34565\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34612\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34750\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34794\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34882\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34967\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16928\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17007\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08111\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08223\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08297\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08333\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08470\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08671\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08768\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09018\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.91355\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82716\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83059\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83610\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83824\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84709\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67707\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59041\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59535\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59886\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60086\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.34045\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25304\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25620\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25857\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25995\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26183\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26403\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26535\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26682\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26786\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26829\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26922\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27046\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27108\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27173\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27226\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27275\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27307\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27353\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27389\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27407\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.33059\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.33171\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.14782\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.05804\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06009\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06107\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06107\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06294\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97297\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.79041\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.79041\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.70122\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.42723\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.33802\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97185\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97331\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88327\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88397\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88465\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79396\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70326\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52138\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43019\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43019\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43019\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43181\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43234\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34238\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34287\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34385\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34432\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34570\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34615\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34703\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34787\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16748\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16827\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07931\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08043\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08117\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08153\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08290\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08491\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08588\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08838\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.91175\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82536\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82879\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83430\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83644\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84056\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84530\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76056\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67527\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58861\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59356\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59706\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59906\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60288\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.33865\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25124\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25440\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25678\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.25815\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26004\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26223\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26355\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26502\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26606\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26650\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26742\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26866\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26928\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26994\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27047\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27095\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27127\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27173\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27209\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27227\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27234\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27234\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27234\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27234\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27234\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27234\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.35019\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.16630\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.07652\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.07857\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.07955\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.07955\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.08142\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.99145\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.80889\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.80889\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.71970\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44571\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.35649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99033\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99179\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90175\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90244\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90312\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81244\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.72113\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.72174\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53986\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44867\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44867\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44867\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45029\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45081\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36085\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36135\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36233\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36280\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36418\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36462\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36550\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36635\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.18596\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.18675\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09779\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09891\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09965\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10001\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10138\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10339\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10436\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10686\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93023\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84384\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84727\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85278\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85492\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85903\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86377\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.77904\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69375\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60709\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.61203\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.61554\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.61754\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.62135\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.35713\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26972\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27288\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27525\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27663\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27851\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28071\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28203\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28349\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28454\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28497\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28714\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28776\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28841\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28894\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28943\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28975\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.29021\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.29057\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.29074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.29082\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.29082\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.29082\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.29082\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.29082\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.29082\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.16547\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.07568\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.07773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.07872\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.07872\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.08059\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.99061\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.80805\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.80805\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.71887\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44488\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.35566\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98950\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99096\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90092\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90161\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90229\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81160\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.72029\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.72091\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53903\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44784\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44784\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44784\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44946\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44998\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36002\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36052\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36150\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36197\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36334\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36379\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36552\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.18513\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.18592\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09695\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09808\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09882\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09917\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10055\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10255\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10353\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10603\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92940\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84301\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84644\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85194\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85409\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85820\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86294\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.77820\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69292\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60626\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.61120\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.61471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.61670\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.62052\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.35629\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.26889\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27205\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27442\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27579\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27768\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.27988\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28119\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28266\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28370\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28507\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28630\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28693\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28758\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28811\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28860\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28892\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28938\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28973\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28991\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.28999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.11230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.11435\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.11533\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.11533\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.11720\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.02723\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.84467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.84467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.75548\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.48149\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.39228\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.02611\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.02757\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.93753\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.93823\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.93891\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.84822\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75691\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75752\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57564\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48445\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48445\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48445\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48607\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48660\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.39664\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.39713\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.39811\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.39858\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.39996\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40041\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40129\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40213\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.22174\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.22253\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13357\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13469\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13543\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13579\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13716\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13917\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.14014\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.14264\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.96601\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87962\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.88305\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.88856\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89070\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89482\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89955\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81482\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72953\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.64287\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.64782\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.65132\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.65332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.65713\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.39291\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.30550\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.30866\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.31104\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.31241\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.31430\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.31649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.31781\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.31928\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32032\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32076\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32168\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32292\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32354\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32419\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32473\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32521\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32553\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32599\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32635\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32653\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32660\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32660\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32660\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32660\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32660\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.32660\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.15149\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.15248\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.15248\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.15435\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.06437\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.88181\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.88181\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.79263\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.51864\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.42942\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06326\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06472\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97468\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97537\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97605\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88536\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79405\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61279\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52160\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52160\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52160\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52322\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52374\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43378\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43428\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43526\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43573\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43710\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43755\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43843\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43928\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25889\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25968\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17071\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17184\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17258\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17293\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17431\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17632\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17979\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00316\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.91677\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92020\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92570\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92785\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93196\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93670\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85196\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76668\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68002\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68496\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68847\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69046\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69428\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.43005\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.34265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.34581\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.34818\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.34955\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.35144\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.35364\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.35495\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.35642\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.35746\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.35790\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.35883\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36069\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36134\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36187\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36268\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36314\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36349\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36367\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36375\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36375\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36375\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36375\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36375\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36375\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.17109\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.17109\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.17296\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.08299\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.90043\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.90043\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.81124\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.53725\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44804\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08187\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08333\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99329\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99399\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90398\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81267\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81328\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63140\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54021\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54021\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54021\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54183\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45240\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45289\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45387\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45434\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45572\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45617\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45704\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45789\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27750\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27829\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.18933\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19045\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19119\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19155\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19292\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19493\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19840\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02177\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93538\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93881\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94432\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94646\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95058\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95531\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87058\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.78529\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69863\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70358\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70708\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70908\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71289\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.44867\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36126\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36442\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36680\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36817\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37225\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37357\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37504\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37608\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37652\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37744\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37868\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37930\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37995\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38049\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38097\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38129\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38175\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38211\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38229\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.17109\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.17296\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.08299\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.90043\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.90043\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.81124\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.53725\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44804\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08187\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08333\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99329\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99399\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90398\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81267\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81328\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63140\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54021\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54021\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54021\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54183\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45240\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45289\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45387\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45434\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45572\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45617\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45704\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45789\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27750\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27829\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.18933\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19045\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19119\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19155\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19292\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19493\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19840\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02177\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93538\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93881\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94432\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94646\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95058\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95531\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87058\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.78529\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69863\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70358\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70708\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70908\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71289\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.44867\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36126\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36442\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36680\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.36817\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37225\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37357\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37504\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37608\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37652\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37744\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37868\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37930\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.37995\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38049\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38097\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38129\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38175\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38211\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38229\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.38236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.21028\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.12031\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.93775\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.93775\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.84857\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.57458\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.48536\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.11919\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12066\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.03061\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.03131\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.03199\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94130\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.84999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.85060\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.66872\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57753\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57753\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57753\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57916\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57968\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48972\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49021\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49119\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49166\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49304\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49349\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49437\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49521\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31482\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31561\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.22665\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.22778\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.22851\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.22887\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.23024\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.23225\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.23322\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.23572\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05909\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.97271\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.97613\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.98164\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.98378\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.98790\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99264\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.90790\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82261\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.73595\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74090\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74441\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74640\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75022\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.48599\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.39858\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.40174\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.40412\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.40549\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.40738\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.40957\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41089\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41340\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41384\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41476\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41600\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41663\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41728\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41781\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41829\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41861\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41943\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41961\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41968\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41968\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41968\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41968\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41968\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.41968\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.15712\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97456\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97456\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.88537\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.61138\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.52216\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15600\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15746\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06742\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06812\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06879\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97811\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88680\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88741\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70553\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61434\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61434\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61434\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61596\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61648\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52653\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52702\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52800\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52847\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52985\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53030\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53117\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53202\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35163\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35242\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26346\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26458\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26532\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26568\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26705\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26906\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27003\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27253\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00951\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01294\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01845\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02059\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02944\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85942\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.77276\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.77770\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.78121\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.78321\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.78702\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52280\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.43539\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.43855\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.44093\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.44230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.44419\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.44638\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.44770\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.44917\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45021\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45064\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45157\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45281\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45343\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45408\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45462\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45510\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45542\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45588\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45624\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45642\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.99200\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.99200\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.90281\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.62882\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.53960\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.17344\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.17490\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08486\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08555\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08623\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99555\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90424\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90485\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.72297\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63340\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63392\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54396\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54446\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54544\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54591\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54861\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54946\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36986\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28090\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28202\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28276\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28312\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28449\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28650\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28747\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28997\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.11334\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02695\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03038\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03589\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03803\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.04214\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.04688\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.96215\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87686\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.79020\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.79514\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.79865\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.80065\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.80446\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54023\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45283\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45599\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45836\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45973\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46162\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46382\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46514\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46660\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46765\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46808\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46901\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47025\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47087\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47152\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47205\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47254\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47286\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47368\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47385\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.99200\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.90281\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.62882\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.53960\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.17344\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.17490\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08486\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08555\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08623\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99555\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90424\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90485\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.72297\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63340\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63392\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54396\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54446\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54544\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54591\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54861\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54946\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36986\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28090\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28202\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28276\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28312\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28449\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28650\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28747\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28997\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.11334\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02695\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03038\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03589\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03803\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.04214\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.04688\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.96215\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87686\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.79020\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.79514\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.79865\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.80065\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.80446\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54023\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45283\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45599\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45836\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.45973\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46162\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46382\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46514\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46660\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46765\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46808\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.46901\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47025\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47087\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47152\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47205\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47254\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47286\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47368\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47385\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.47393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.95843\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.68444\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.59522\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.22906\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.23052\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.14047\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.14117\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.14185\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.05116\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.95985\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96046\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77859\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68739\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68739\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68739\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68902\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68954\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.59958\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60007\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60105\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60153\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60290\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60335\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60423\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60508\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.42468\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.42547\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33651\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33764\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33838\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33873\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34010\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34211\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34308\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34559\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16895\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08257\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08599\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09150\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09365\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09776\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10250\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01776\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93248\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.84582\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85076\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85427\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85626\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86008\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59585\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.50844\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51160\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51398\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51535\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51724\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.51943\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52075\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52326\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52370\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52462\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52586\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52714\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52767\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52815\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52848\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52894\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52929\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52947\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52954\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52954\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52954\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52954\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52954\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52954\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.70094\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.61172\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.24556\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.24702\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15698\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15768\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15836\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06767\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97636\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97697\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79509\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70390\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70390\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70390\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70552\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70605\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61609\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61658\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61756\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61803\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61941\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61986\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62073\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62158\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44119\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44198\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35302\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35488\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35524\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35661\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35862\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35959\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36209\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.18546\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10250\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10801\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.11015\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.11427\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.11900\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03427\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94898\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86232\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86727\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87077\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87277\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87658\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.61236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52495\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.52811\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.53049\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.53186\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.53375\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.53594\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.53726\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.53873\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.53977\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54021\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54113\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54237\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54299\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54364\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54418\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54466\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54498\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54544\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54580\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54598\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54605\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54605\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54605\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54605\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54605\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.54605\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.66737\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.30121\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.30267\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.21263\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.21333\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.21400\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.03201\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.03262\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.85074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75955\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75955\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75955\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76117\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76169\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67174\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67223\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67321\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67368\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67506\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67551\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67638\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67723\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49684\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49763\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40867\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40979\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41053\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41089\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41226\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41427\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41524\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41774\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24111\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.15472\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.15815\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16366\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16580\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16992\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17465\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08992\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00463\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.91797\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92292\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92642\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92842\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93223\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.66801\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58060\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58376\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58614\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58751\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58940\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59159\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59291\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59438\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59542\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59586\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59678\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59802\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59864\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59929\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59983\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60031\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60063\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60109\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60145\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60163\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60170\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60170\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60170\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60170\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60170\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.60170\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.29793\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.29939\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20935\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.21005\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.21073\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12004\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.02873\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.02934\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.84746\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75627\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75627\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75627\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75789\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75842\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.66846\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.66895\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.66993\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67040\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67223\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67311\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67395\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49356\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49435\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40539\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40651\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40725\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40761\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40898\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41099\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41196\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41446\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.23783\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.15145\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.15487\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16038\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16252\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16664\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17138\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08664\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00135\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.91469\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.91964\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92315\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92514\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92896\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.66473\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.57732\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58048\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58286\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58423\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58612\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58831\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.58963\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59110\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59214\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59258\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59350\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59474\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59537\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59602\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59655\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59703\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59735\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59781\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59817\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59835\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59842\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59842\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59842\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59842\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59842\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.59842\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.33713\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.24709\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.24778\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.24846\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15777\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06646\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06708\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88520\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79400\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79400\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79400\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79563\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79615\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70619\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70668\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70767\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70814\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70951\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70996\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71084\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71169\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53130\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53208\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44312\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44425\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44499\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44534\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44672\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44872\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44970\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45220\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27557\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.18918\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19261\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19811\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.20026\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.20437\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.20911\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.12437\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03909\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95243\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95737\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.96088\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.96287\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.96669\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70246\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.61505\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.61822\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.62059\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.62196\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.62385\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.62605\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.62736\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.62883\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.62987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63031\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63124\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63247\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63310\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63375\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63428\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63477\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63509\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63555\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63608\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63616\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63616\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63616\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63616\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63616\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.63616\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.28396\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.28466\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.28534\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.19465\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.10334\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.10395\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92207\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83088\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83088\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83088\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83250\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83303\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74307\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74356\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74454\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74501\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74639\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74684\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74772\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74856\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56817\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56896\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48000\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48112\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48186\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48359\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48560\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48657\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31244\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.22605\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.22948\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.23499\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.23713\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24125\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24599\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16125\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07596\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.98930\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99425\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99775\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99975\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00357\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.73934\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.65193\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.65509\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.65747\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.65884\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.66073\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.66292\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.66424\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.66571\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.66675\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.66719\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.66811\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.66935\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.66997\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67063\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67116\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67164\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67196\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67242\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67278\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67296\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67303\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67303\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67303\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67303\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67303\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67303\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.30356\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.30424\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.21355\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12224\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12285\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94097\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.84978\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.84978\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.84978\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.85141\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.85193\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76197\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76246\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76344\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76391\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76529\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76574\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76662\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76746\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.58707\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.58786\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49890\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50003\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50076\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50112\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50249\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50450\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50547\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50797\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33134\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24496\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24838\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25389\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25603\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26015\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.18015\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09486\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00820\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01315\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01666\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01865\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02247\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75824\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67083\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67399\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67637\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67774\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.67963\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68182\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68314\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68461\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68565\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68609\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68701\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68825\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68888\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68953\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69054\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69086\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69132\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69168\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69186\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69193\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69193\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69193\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69193\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69193\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69193\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.32316\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.23247\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.14116\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.14177\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.95989\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.86870\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.86870\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.86870\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87032\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87085\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.78089\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.78138\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.78236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.78283\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.78421\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.78466\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.78554\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.78638\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60599\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60678\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.51782\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.51894\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.51968\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52004\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52141\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52342\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52439\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52689\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35026\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26387\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26730\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27281\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27495\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28380\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.11378\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02712\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03207\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03557\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.03757\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.04138\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.77716\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.68975\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69291\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69529\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69666\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.69855\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70206\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70353\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70457\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70501\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70593\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70717\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70779\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70844\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70898\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70946\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70978\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71024\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71060\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71078\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71085\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71085\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71085\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71085\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71085\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71085\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.25039\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15908\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15969\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97782\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88662\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88662\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88662\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88825\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88877\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79881\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79930\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80029\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80076\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80213\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80258\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80346\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80431\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62392\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62470\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53574\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53687\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53761\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53796\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53933\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54134\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54232\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54482\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36818\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28180\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28523\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29073\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29288\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29699\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30173\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.21699\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13171\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.04505\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.04999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05350\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05549\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05931\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.79508\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70767\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71083\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71321\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71458\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71647\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71866\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71998\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72145\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72249\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72293\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72386\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72509\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72572\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72637\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72690\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72738\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72771\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72817\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72852\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72870\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15803\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15865\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97677\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88557\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88557\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88557\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88720\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88772\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79776\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79825\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79924\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79971\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80108\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80153\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80241\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80326\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62287\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62365\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53469\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53582\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53656\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53691\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53828\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54029\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LOSS=1.54127\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54377\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36714\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28075\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28418\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28968\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29183\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29594\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30068\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.21594\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13066\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.04400\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.04894\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05245\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05444\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05826\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.79403\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70662\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.70978\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71216\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71353\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71542\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71761\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.71893\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72040\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72144\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72188\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72281\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72404\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72532\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72585\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72633\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72666\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72712\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72747\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72765\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.17763\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99575\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90456\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90456\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90456\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90618\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90670\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81675\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81724\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81822\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81869\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.82007\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.82051\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.82139\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.82224\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.64185\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.64264\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55368\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55480\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55554\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55727\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55928\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56025\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56275\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.38612\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29973\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30316\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30867\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31081\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31493\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31966\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.23493\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.14964\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06298\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06792\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07143\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07343\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07724\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81302\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72561\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.72877\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.73115\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.73252\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.73441\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.73660\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.73792\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.73939\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74043\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74086\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74179\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74303\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74365\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74430\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74483\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74532\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74564\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74610\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74646\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74664\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74671\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74671\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74671\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74671\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74671\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74671\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.01251\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92131\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92131\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92131\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92294\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92346\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83350\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83399\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83498\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83545\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83682\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83727\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83815\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83900\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65861\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65939\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57043\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57156\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57403\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57603\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57701\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57951\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40288\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31992\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32542\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32757\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33168\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33642\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25168\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16640\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07974\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08468\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08819\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09018\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09400\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82977\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74553\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74790\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74927\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75116\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75336\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75614\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75718\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75762\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75855\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75978\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76041\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76106\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76159\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76208\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76240\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76286\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76321\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76339\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76347\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76347\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76347\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76347\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76347\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76347\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92015\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92015\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92015\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92177\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92229\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83233\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83283\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83428\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83566\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83610\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83698\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83783\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65744\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65823\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56926\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57039\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57113\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57149\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57286\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57487\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57584\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57834\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40171\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31532\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31875\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32426\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32640\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33051\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33525\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25052\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16523\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07857\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08351\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08702\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08902\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09283\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82860\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74120\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74436\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74673\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74810\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75219\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75351\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75497\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75602\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75645\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75738\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75861\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75924\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75989\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76042\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76091\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76123\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76169\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76204\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92015\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92015\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92177\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92229\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83233\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83283\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83428\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83566\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83610\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83698\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83783\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65744\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65823\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56926\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57039\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57113\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57149\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57286\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57487\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57584\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57834\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40171\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31532\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31875\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32426\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32640\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33051\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33525\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25052\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16523\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07857\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08351\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08702\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08902\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09283\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82860\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74120\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74436\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74673\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74810\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75219\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75351\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75497\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75602\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75645\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75738\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75861\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75924\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75989\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76042\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76091\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76123\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76169\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76204\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92015\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92177\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92229\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83233\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83283\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83428\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83566\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83610\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83698\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83783\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65744\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65823\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56926\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57039\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57113\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57149\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57286\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57487\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57584\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57834\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40171\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31532\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31875\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32426\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32640\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33051\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33525\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25052\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16523\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07857\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08351\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08702\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08902\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09283\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82860\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74120\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74436\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74673\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74810\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.74999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75219\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75351\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75497\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75602\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75645\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75738\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75861\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75924\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.75989\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76042\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76091\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76123\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76169\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76204\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.76230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97894\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97946\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88950\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89098\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89145\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89282\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89327\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89415\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89500\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71461\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71539\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62643\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62756\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62830\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62865\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63003\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63203\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63301\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63551\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45888\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.37249\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.37592\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.38142\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.38357\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.38768\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.39242\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30768\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.22240\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13574\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.14068\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.14419\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.14618\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.15000\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.88577\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.79837\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.80153\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.80390\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.80527\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.80716\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.80936\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81067\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81214\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81318\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81362\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81455\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81578\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81641\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81706\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81759\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81808\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81840\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81886\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81921\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81939\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81947\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81947\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81947\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81947\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81947\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81947\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99853\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90858\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91005\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91052\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91190\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91235\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91322\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91407\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.73368\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.73447\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.64551\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.64663\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.64737\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.64773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.64910\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65111\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65208\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65458\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.47795\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.39156\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.39499\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40050\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40264\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.40676\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41149\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32676\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24147\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.15481\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.15976\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16326\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16526\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.90485\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.81744\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82060\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82298\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82435\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82624\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82843\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.82975\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83122\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83226\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83270\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83362\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83486\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83548\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83613\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83667\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83715\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83747\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83793\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83829\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83847\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83854\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83854\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83854\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83854\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83854\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.83854\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94537\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94586\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94684\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94731\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94869\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94914\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.95002\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.95086\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77047\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77126\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68343\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68416\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68452\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68589\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68790\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68887\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.69137\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.51474\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.42836\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43943\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44355\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44829\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36355\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27826\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19160\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.19655\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.20006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.20205\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.20587\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94164\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85423\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85739\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.85977\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86114\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86303\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86522\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86654\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86801\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86905\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.86949\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87041\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87165\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87228\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87293\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87346\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87394\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87426\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87472\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87508\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87526\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87533\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87533\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87533\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87533\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87533\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87533\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96497\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96595\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96642\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96779\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96824\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96912\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96997\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.78958\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79037\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70140\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70253\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70327\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70362\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70500\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70701\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70798\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71048\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.53385\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44746\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45089\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45639\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45854\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.46265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.46739\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.38265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29737\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.21071\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.21565\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.21916\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.22115\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.22497\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.96074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87334\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87650\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.87887\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.88024\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.88213\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.88433\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.88564\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.88711\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.88815\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.88859\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.88952\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89075\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89138\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89203\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89256\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89305\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89337\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89383\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89418\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89436\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89444\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89444\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89444\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89444\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89444\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89444\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.89444\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.00416\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.00463\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.00601\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.00645\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.00733\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.00818\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.82779\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.82858\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.73962\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74148\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74184\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74321\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74522\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74619\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74869\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57206\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48567\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.48910\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49461\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49675\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50087\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50560\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.42087\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33558\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24892\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25386\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25737\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25937\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26318\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99896\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.91155\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.91471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.91708\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.91846\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92034\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92254\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92386\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92533\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92637\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92680\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92897\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.92959\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93024\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93077\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93126\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93158\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93204\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93240\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93258\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.02376\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.02513\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.02558\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.02646\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.02731\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.84692\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.84770\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75874\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76061\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76096\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76233\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76434\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76532\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76782\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.59119\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50480\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50823\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.51373\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.51588\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.51999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.52473\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.26805\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27299\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27650\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.27849\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28231\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01808\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93067\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93384\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93621\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93758\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.93947\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94167\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94298\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94445\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94549\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94593\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94686\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94809\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94872\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94937\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.94990\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95038\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95071\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95117\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95152\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95170\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.95178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08255\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08299\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08387\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08472\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90433\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90512\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81616\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81728\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81802\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81838\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81975\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.82176\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.82273\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.82523\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.64860\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56221\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56564\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57115\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57329\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57741\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.58214\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49741\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41212\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32546\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33040\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33391\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33591\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33972\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.07550\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.98809\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99125\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99363\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99500\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99689\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=0.99908\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00040\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00187\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00291\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00334\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00427\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00551\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00613\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00678\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00731\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00780\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00812\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00858\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00894\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00912\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00919\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00919\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00919\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00919\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00919\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00919\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.10214\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.10302\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.10387\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92348\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92427\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83531\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83643\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83717\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83753\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83890\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.84091\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.84188\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.84438\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.66775\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.58136\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.58479\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.59030\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.59244\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.59656\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60129\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.51656\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43127\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34461\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34955\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35306\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35506\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35887\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09465\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.00724\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01040\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01278\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01415\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01604\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01823\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.01955\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02102\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02206\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02249\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02342\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02466\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02528\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02593\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02646\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02695\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02727\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02809\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02827\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02834\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02834\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02834\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02834\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02834\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.02834\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.14134\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.14219\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96180\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96258\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87362\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87475\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87549\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87584\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87722\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87922\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88020\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88270\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70607\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61968\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62311\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62861\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63076\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63487\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63961\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55487\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.46959\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.38293\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.38787\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.39138\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.39337\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.39719\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13296\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.04555\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.04872\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05109\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05246\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05435\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05655\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05786\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.05933\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06037\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06081\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06174\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06297\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06360\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06425\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06478\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06527\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06559\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06605\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06640\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06658\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06666\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06666\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06666\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06666\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06666\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.06666\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.18053\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.00014\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.00093\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91197\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91309\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91383\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91419\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91556\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91757\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91854\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92104\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74441\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65802\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.66145\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.66696\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.66910\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67322\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67795\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.59322\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50793\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.42127\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.42622\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.42972\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43172\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43553\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17131\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08390\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08706\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.08944\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09081\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09270\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09621\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09768\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09872\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.09916\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10008\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10132\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10194\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10259\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10313\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10361\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10393\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10439\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10475\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10493\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10500\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10500\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10500\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10500\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10500\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.10500\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.03501\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.03580\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94683\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94796\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94870\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94905\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.95043\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.95244\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.95341\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.95591\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77928\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.69289\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.69632\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70183\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70397\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.70808\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71282\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62808\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54280\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.45614\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.46108\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.46459\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.46658\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.47040\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.20617\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.11877\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.12193\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.12430\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.12567\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.12756\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.12976\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13107\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13254\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13358\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13402\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13495\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13618\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13681\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13746\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13799\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13848\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13880\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13926\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13961\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13979\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.13987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.07420\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98524\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98636\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98710\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98746\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98883\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99084\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99181\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99431\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81768\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.73129\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.73472\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74023\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74237\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75122\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.66649\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.58120\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49454\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.49949\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50299\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50499\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.50880\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24458\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.15717\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16033\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16271\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16408\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16597\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16816\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.16948\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17095\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17199\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17243\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17335\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17459\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17521\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17586\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17640\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17688\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17720\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17766\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17802\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17820\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17827\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17827\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17827\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17827\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17827\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.17827\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06023\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06135\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06209\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06245\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06382\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06583\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06680\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.06930\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89267\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80628\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.80971\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81522\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.81736\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.82148\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.82621\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.74148\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.65619\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56953\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57448\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57798\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.57998\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.58379\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31957\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.23216\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.23532\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.23770\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.23907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24096\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24315\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24447\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24594\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24698\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24742\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24834\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.24958\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25020\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25085\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25139\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25187\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25219\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25301\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25319\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25326\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25326\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25326\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25326\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25326\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.25326\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.11902\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.11976\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12012\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12149\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12350\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12447\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12697\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.95034\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.86395\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.86738\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87289\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87503\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87915\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88388\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.79915\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.71386\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62720\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63214\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63565\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.63765\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.64146\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.37724\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.28983\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29299\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29536\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29674\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.29862\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30082\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30214\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30360\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30465\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30508\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30601\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30725\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30787\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30852\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30905\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30954\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.30986\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31032\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31068\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31085\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31093\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31093\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31093\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31093\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31093\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.31093\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15821\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15857\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.15994\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.16195\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.16292\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.16542\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98879\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90241\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.90583\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91134\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91348\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.91760\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92234\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83760\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75231\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.66565\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67060\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67411\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67610\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.67992\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.41569\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.32828\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33144\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33382\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33519\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33708\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.33927\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34059\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34206\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34310\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34354\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34446\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34570\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34633\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34698\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34751\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34799\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34831\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34877\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34913\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34931\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34938\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34938\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34938\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34938\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34938\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34938\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.17781\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.17918\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.18119\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.18216\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.18466\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.00803\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92165\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.92507\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.93058\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.93273\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.93684\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94158\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.85684\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77156\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68984\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.69335\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.69534\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.69916\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43493\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.34752\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35068\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35306\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35443\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35632\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35851\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.35983\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36130\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36234\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36278\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36370\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36494\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36557\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36622\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36675\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36723\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36756\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36802\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36837\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36855\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36862\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36862\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36862\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36862\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36862\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.36862\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.25620\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.25821\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.25918\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.26168\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08505\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.99866\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.00209\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.00760\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.00974\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.01386\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.01859\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.93386\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.84857\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76191\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76685\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77036\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77617\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.51195\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.42454\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.42770\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43008\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43145\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43334\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43553\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43685\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43832\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43936\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.43979\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44072\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44196\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44258\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44323\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44376\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44425\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44457\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44503\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44539\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44557\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44564\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44564\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44564\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44564\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44564\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.44564\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.37378\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.37475\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.37725\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20062\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.11424\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.11766\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12317\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12531\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.12943\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.13417\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.04943\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.87748\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88243\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88594\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.88793\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.89175\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.62752\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54011\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54327\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54565\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54702\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.54891\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55110\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55242\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55389\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55493\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55537\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55629\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55753\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55816\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55881\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55934\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.55982\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56014\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56060\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56096\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56114\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56121\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56121\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56121\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56121\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56121\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.56121\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43257\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43507\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.25844\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.17205\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.17548\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.18099\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.18313\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.18725\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.19198\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.10725\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.02196\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.93530\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94025\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94375\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94575\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.94956\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.68534\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.59793\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60109\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60347\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60484\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60673\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.60892\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61024\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61171\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61275\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61319\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61411\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61535\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61597\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61662\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61716\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61764\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61796\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61842\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61896\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61903\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61903\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61903\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61903\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61903\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.61903\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.58935\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.41272\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.32633\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.32976\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.33527\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.33741\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.34152\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.34626\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.26153\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.17624\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.08958\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.09452\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.09803\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.10003\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.10384\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.83961\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75221\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75537\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75774\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.75911\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76100\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76320\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76452\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76598\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76703\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76746\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76839\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.76963\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77025\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77090\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77143\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77192\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77224\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77270\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77306\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77323\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77331\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77331\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77331\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77331\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77331\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.77331\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.62020\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.53381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.53724\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.54274\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.54489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.54900\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.55374\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.46900\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.38372\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.29706\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.30200\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.30551\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.30750\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.31132\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.04709\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.95969\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96285\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96522\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96659\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.96848\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97068\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97199\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97346\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97450\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97494\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97587\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97710\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97838\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97891\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97940\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.97972\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98018\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98053\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98071\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98079\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98079\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98079\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98079\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98079\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=1.98079\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.76300\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.76643\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.77194\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.77408\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.77819\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.78293\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.69820\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.61291\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.52625\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.53119\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.53470\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.53670\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.54051\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.27628\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.18888\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.19204\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.19441\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.19578\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.19767\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.19987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20119\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20265\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20370\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20413\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20506\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20630\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20692\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20757\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20810\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20859\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20891\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20937\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20973\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20990\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20998\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20998\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20998\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20998\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20998\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.20998\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.99816\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.00367\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.00582\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.00993\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.01467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.92993\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.84465\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.75799\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.76293\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.76644\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.76843\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.77225\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.50802\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.42061\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.42377\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.42615\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.42752\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.42941\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43160\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43292\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43439\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43543\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43587\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43679\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43803\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43866\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43931\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.43984\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44032\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44065\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44111\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44146\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44164\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44172\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44172\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44172\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44172\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44172\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.44172\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.39011\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.39225\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.39636\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.40110\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.31637\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.23108\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.14442\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.14936\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.15287\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.15486\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.15868\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.89445\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.80705\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.81021\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.81258\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.81395\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.81584\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.81804\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.81935\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82082\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82187\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82323\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82446\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82509\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82574\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82627\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82676\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82708\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82754\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82789\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82807\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82815\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82815\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82815\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82815\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82815\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.82815\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.54688\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.55100\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.55573\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.47100\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.38571\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29905\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.30400\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.30750\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.30950\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.31331\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.04909\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.96168\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.96484\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.96722\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.96859\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97048\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97267\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97399\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97546\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97650\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97693\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97786\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97910\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.97972\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98037\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98091\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98139\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98171\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98217\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98253\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98271\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98278\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98278\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98278\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98278\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98278\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=2.98278\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.86043\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.86517\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.78044\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.69515\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.60849\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.61343\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.61694\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.61894\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.62275\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.35852\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.27112\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.27428\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.27665\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.27802\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.27991\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.28211\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.28343\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.28489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.28594\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.28637\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.28730\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.28853\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.28916\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.28981\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29034\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29083\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29115\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29161\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29196\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29214\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.29222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.23278\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.14804\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.06276\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.97610\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.98104\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.98455\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.98654\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.99036\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.72613\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.63872\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.64188\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.64426\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.64563\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.64752\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.64971\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65103\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65250\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65354\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65398\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65491\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65614\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65677\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65742\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65795\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65843\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65876\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65922\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65957\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65975\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65983\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65983\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65983\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65983\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65983\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=3.65983\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.53236\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.44707\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.36041\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.36536\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.36886\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.37086\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.37467\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.11045\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.02304\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.02620\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.02858\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.02995\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.03184\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.03403\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.03535\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.03682\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.03786\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.03829\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.03922\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04046\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04108\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04173\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04227\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04275\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04307\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04353\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04389\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04407\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.04414\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.79275\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.70608\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.71103\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.71454\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.71653\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.72035\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.45612\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.36871\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.37187\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.37425\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.37562\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.37751\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.37970\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38102\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38249\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38353\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38397\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38613\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38676\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38741\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38794\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38842\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38875\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38921\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38956\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38974\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38981\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38981\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38981\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38981\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38981\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.38981\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.93555\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.94049\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.94400\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.94600\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.94981\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.68559\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.59818\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.60134\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.60371\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.60509\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.60697\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.60917\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61049\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61195\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61300\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61343\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61436\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61560\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61622\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61687\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61740\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61789\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61821\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61867\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61903\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61920\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61928\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61928\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61928\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61928\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61928\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=4.61928\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.38628\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.38979\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39178\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39560\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.13137\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.04397\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.04713\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.04950\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.05087\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.05276\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.05496\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.05627\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.05774\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.05878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.05922\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06015\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06138\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06201\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06266\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06319\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06368\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06400\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06446\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06481\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06499\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06507\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06507\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06507\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06507\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06507\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.06507\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.71943\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.72143\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.72524\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.46102\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.37361\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.37677\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.37914\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.38052\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.38240\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.38460\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.38592\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.38738\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.38843\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.38886\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.38979\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39103\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39165\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39230\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39283\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39332\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39364\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39410\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39446\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39463\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.39471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.91540\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.91922\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.65499\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.56758\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.57074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.57312\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.57449\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.57638\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.57857\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.57989\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58136\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58240\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58284\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58376\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58500\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58563\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58628\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58681\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58762\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58808\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58843\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58861\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58868\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58868\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58868\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58868\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58868\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58868\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.58868\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.30734\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.04312\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.95571\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.95887\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.96124\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.96262\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.96450\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.96670\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.96802\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.96949\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97053\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97096\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97189\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97313\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97375\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97440\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97493\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97542\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97574\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97620\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97656\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97674\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97681\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97681\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97681\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97681\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97681\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97681\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=5.97681\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.30462\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.21721\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.22037\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.22275\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.22412\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.22601\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.22820\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.22952\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23099\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23203\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23247\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23339\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23463\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23526\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23591\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23644\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23692\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23724\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23770\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23806\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23824\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23831\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23831\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23831\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23831\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23831\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.23831\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.36904\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.37220\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.37457\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.37594\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.37783\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38003\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38134\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38281\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38386\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38429\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38522\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38645\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38708\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38826\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38875\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38953\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.38988\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.39006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.39014\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.39014\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.39014\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.39014\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.39014\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.39014\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.74138\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.74376\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.74513\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.74702\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.74921\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75053\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75200\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75304\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75347\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75440\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75564\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75626\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75691\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75745\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75793\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75825\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75871\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75907\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75925\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75932\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75932\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75932\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75932\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75932\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=6.75932\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.03534\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.03671\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.03860\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.04079\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.04211\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.04358\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.04462\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.04505\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.04598\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.04722\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.04784\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.04849\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.04903\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.04951\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.04983\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.05029\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.05065\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.05083\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.05090\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.05090\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.05090\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.05090\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.05090\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.05090\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.21171\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.21360\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.21579\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.21711\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.21858\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.21962\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22098\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22222\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22284\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22349\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22403\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22451\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22483\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22529\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22565\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22583\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.22590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.46647\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.46866\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.46998\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47145\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47249\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47293\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47386\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47509\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47572\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47637\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47690\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47738\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47771\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47817\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47852\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47870\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.47878\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78002\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78134\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78281\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78385\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78429\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78521\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78645\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78708\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78773\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78826\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78874\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78906\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78952\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.78988\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.79006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.79013\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.79013\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.79013\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.79013\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.79013\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.79013\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.97599\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.97746\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.97850\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.97894\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.97987\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98110\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98173\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98238\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98291\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98340\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98372\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98418\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98453\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98471\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98479\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98479\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98479\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98479\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98479\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=7.98479\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21116\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21220\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21264\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21356\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21480\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21542\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21607\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21661\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21709\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21741\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21787\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21823\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21841\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21848\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21848\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21848\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21848\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21848\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.21848\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.38753\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.38797\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.38889\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39013\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39076\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39141\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39194\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39242\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39274\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39320\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39356\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39374\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.39381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.46592\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.46685\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.46808\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.46871\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.46936\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.46989\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.47037\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.47070\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.47116\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.47151\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.47169\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.47177\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.47177\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.47177\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.47177\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.47177\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.47177\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64229\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64353\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64415\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64481\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64534\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64582\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64614\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64660\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64696\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64714\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64721\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64721\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64721\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64721\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64721\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64721\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.64721\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.89705\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.89768\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.89833\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.89886\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.89935\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.89967\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.90013\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.90048\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.90066\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.90074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.90074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.90074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.90074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.90074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.90074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=8.90074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03423\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03488\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03542\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03590\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03622\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03668\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03704\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03722\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.03729\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19101\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19154\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19203\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19235\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19281\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19316\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19334\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19342\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19342\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19342\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19342\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19342\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.19342\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.32819\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.32867\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.32900\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.32946\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.32981\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.32999\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.33006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.33006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.33006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.33006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.33006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.33006\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.46537\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.46569\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.46615\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.46651\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.46668\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.46676\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.46676\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.46676\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.46676\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.46676\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.46676\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.56335\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.56381\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.56417\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.56435\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.56442\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.56442\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.56442\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.56442\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.56442\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.56442\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.72013\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.72049\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.72066\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.72074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.72074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.72074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.72074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.72074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.72074\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.85731\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.85749\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.85756\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.85756\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.85756\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.85756\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.85756\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.85756\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.93570\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.93577\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.93577\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.93577\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.93577\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.93577\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.93577\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "TYPE: (617, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      ">LOSS=9.97489\n",
      "best:\n",
      " [[0.06, 0.7399999999999997, 0.16849639412303927], [0.05, 0.7400000000000001, 0.16956042690491901], [0.04, 0.7400000000000001, 0.17097486966070308], [0.03, 0.7400000000000001, 0.17150268378531375], [0.06, 0.7499999999999997, 0.1716567104622406], [0.02, 0.7399999999999999, 0.17221991832540434], [0.01, 0.74, 0.17240132131804956], [0.0, 0.74, 0.17246727287675184], [0.05, 0.7500000000000001, 0.1727207432441203], [0.06, 0.7599999999999997, 0.17403244438707113]]\n"
     ]
    }
   ],
   "source": [
    "best_thres = evaluation(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3cb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35cb8e2d",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7350af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing\n",
    "Id, test = prepair_test(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d566ce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE: (20, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      "TYPE: (20, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      "TYPE: (20, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      "TYPE: (20, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      "TYPE: (20, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "num_ensembles = len(models)\n",
    "y_p_lst = list()\n",
    "\n",
    "for ensemble in models:\n",
    "    y_pred = ensemble.predict_proba(test)\n",
    "    shape = test.shape[0]\n",
    "    y_p = calibrate_prob(y_pred, shape, best_thres[0], best_thres[1])\n",
    "    y_p_lst.append(y_p)\n",
    "\n",
    "y_p = np.array(y_p_lst).sum(axis=0)/num_ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ee1ed742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#post processing\n",
    "shape = test.shape[0]\n",
    "\n",
    "submission = pd.DataFrame(Id, columns=['Id'])\n",
    "submission[\"class_0\"] = y_p[:, 0]\n",
    "submission[\"class_1\"] = y_p[:, 1]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "93908c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07226396249497796"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = balanced_log_loss(a.Class, y_p)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d0330221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9b9e6e21b830</td>\n",
       "      <td>0.766163</td>\n",
       "      <td>0.233837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b358f0fdb95b</td>\n",
       "      <td>0.583543</td>\n",
       "      <td>0.416457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ffcca4ded3bb</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d694ba9aa16</td>\n",
       "      <td>0.014357</td>\n",
       "      <td>0.985643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ba0c42e12f1f</td>\n",
       "      <td>0.733145</td>\n",
       "      <td>0.266855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4c92a971cf91</td>\n",
       "      <td>0.699596</td>\n",
       "      <td>0.300404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>da1a5aec1818</td>\n",
       "      <td>0.049112</td>\n",
       "      <td>0.950888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1a9fecf65695</td>\n",
       "      <td>0.829510</td>\n",
       "      <td>0.170490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>332a3850302f</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2cddd2d70d72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a082f422799b</td>\n",
       "      <td>0.057733</td>\n",
       "      <td>0.942267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b85846bebd78</td>\n",
       "      <td>0.942038</td>\n",
       "      <td>0.057962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0d1b855c7635</td>\n",
       "      <td>0.681450</td>\n",
       "      <td>0.318550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dd65e067165c</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>174ba5dc00b1</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.330579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7cc870296984</td>\n",
       "      <td>0.828906</td>\n",
       "      <td>0.171094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>703c183d0c6c</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>af2c482fb8d7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2fd659800f75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0b2cc0b0e6c5</td>\n",
       "      <td>0.654268</td>\n",
       "      <td>0.345732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id   class_0   class_1\n",
       "0   9b9e6e21b830  0.766163  0.233837\n",
       "1   b358f0fdb95b  0.583543  0.416457\n",
       "2   ffcca4ded3bb  1.000000  0.000000\n",
       "3   5d694ba9aa16  0.014357  0.985643\n",
       "4   ba0c42e12f1f  0.733145  0.266855\n",
       "5   4c92a971cf91  0.699596  0.300404\n",
       "6   da1a5aec1818  0.049112  0.950888\n",
       "7   1a9fecf65695  0.829510  0.170490\n",
       "8   332a3850302f  1.000000  0.000000\n",
       "9   2cddd2d70d72  0.000000  1.000000\n",
       "10  a082f422799b  0.057733  0.942267\n",
       "11  b85846bebd78  0.942038  0.057962\n",
       "12  0d1b855c7635  0.681450  0.318550\n",
       "13  dd65e067165c  1.000000  0.000000\n",
       "14  174ba5dc00b1  0.669421  0.330579\n",
       "15  7cc870296984  0.828906  0.171094\n",
       "16  703c183d0c6c  1.000000  0.000000\n",
       "17  af2c482fb8d7  1.000000  0.000000\n",
       "18  2fd659800f75  1.000000  0.000000\n",
       "19  0b2cc0b0e6c5  0.654268  0.345732"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission = pd.read_csv('submission.csv')\n",
    "df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5922c314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0ab65e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('../datasets/icr-identify-age-related-conditions/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b26643e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fed81962",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = a.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "72fb7e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 57)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "466317e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n",
      "/Users/hadangvu/anaconda3/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n"
     ]
    }
   ],
   "source": [
    "Id , test = prepair_test(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c0eae8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.176437112832583"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_thres[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "19ab1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = test.Class\n",
    "test = test.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c8cbe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0150184  0.97660683 0.00475821 0.00361655]\n",
      " [0.91013772 0.03044561 0.04216328 0.01725339]\n",
      " [0.23939487 0.74559242 0.00799461 0.0070181 ]\n",
      " [0.71980441 0.10288747 0.08039475 0.09691337]\n",
      " [0.66116159 0.05957084 0.06742329 0.21184428]\n",
      " [0.04558401 0.93268208 0.0125292  0.00920471]\n",
      " [0.92986021 0.0236091  0.03116548 0.01536521]\n",
      " [0.61989514 0.27334312 0.06626983 0.04049191]\n",
      " [0.70154113 0.04703769 0.12069711 0.13072407]\n",
      " [0.67498496 0.28656156 0.0244525  0.01400098]\n",
      " [0.67974516 0.11320133 0.02249874 0.18455477]\n",
      " [0.60997036 0.12503957 0.02607852 0.23891156]\n",
      " [0.54718602 0.2776101  0.00920867 0.16599521]\n",
      " [0.01136154 0.92890694 0.05503061 0.00470092]\n",
      " [0.74143032 0.06579484 0.10255189 0.09022295]\n",
      " [0.42182102 0.49280779 0.07745074 0.00792046]\n",
      " [0.67643141 0.11520324 0.04150181 0.16686354]\n",
      " [0.61999488 0.316123   0.03056494 0.03331717]\n",
      " [0.60484306 0.19577769 0.11170487 0.08767438]\n",
      " [0.82758634 0.14467183 0.01418133 0.0135605 ]\n",
      " [0.90231939 0.03535361 0.03082581 0.0315012 ]\n",
      " [0.85446614 0.05256472 0.06373844 0.0292307 ]\n",
      " [0.05615976 0.00806424 0.92630943 0.00946657]\n",
      " [0.66150174 0.16991732 0.10934776 0.05923317]\n",
      " [0.00622503 0.98521279 0.00296028 0.00560189]\n",
      " [0.70879186 0.06529252 0.11367834 0.11223728]\n",
      " [0.58930797 0.26703406 0.03054932 0.11310864]\n",
      " [0.65326156 0.32369154 0.01222913 0.01081777]\n",
      " [0.65662235 0.18884687 0.1343894  0.02014139]\n",
      " [0.72288695 0.04542174 0.18157225 0.05011905]\n",
      " [0.66339274 0.02464213 0.20392393 0.1080412 ]\n",
      " [0.02125246 0.92416392 0.05043734 0.00414628]\n",
      " [0.78904342 0.04245707 0.04026214 0.12823737]\n",
      " [0.77619096 0.05753678 0.03528315 0.1309891 ]\n",
      " [0.73852636 0.08936205 0.11930223 0.05280936]\n",
      " [0.01972748 0.0628909  0.12576528 0.79161634]\n",
      " [0.69637893 0.12246534 0.01999339 0.16116234]\n",
      " [0.76171303 0.06495886 0.12144954 0.05187858]\n",
      " [0.64153721 0.19851344 0.0301928  0.12975656]\n",
      " [0.67338312 0.02890979 0.26125626 0.03645083]\n",
      " [0.62479614 0.14289637 0.0300928  0.20221469]\n",
      " [0.62395687 0.02971946 0.13412149 0.21220218]\n",
      " [0.01295216 0.97595695 0.00493077 0.00616012]\n",
      " [0.86785883 0.04669153 0.04674648 0.03870316]\n",
      " [0.59724288 0.21716055 0.11163353 0.07396303]\n",
      " [0.93111725 0.02632326 0.02024307 0.02231642]\n",
      " [0.79143352 0.04648907 0.03702486 0.12505254]\n",
      " [0.32129185 0.11996596 0.1315692  0.42717299]\n",
      " [0.62924645 0.21746192 0.0534523  0.09983934]\n",
      " [0.77455585 0.0874493  0.08628323 0.05171162]\n",
      " [0.68202845 0.20231441 0.03822901 0.07742813]\n",
      " [0.54501728 0.02608711 0.32721649 0.10167912]\n",
      " [0.6792936  0.0308642  0.24113653 0.04870568]\n",
      " [0.61990519 0.3503593  0.01651895 0.01321656]\n",
      " [0.75553595 0.03532165 0.04154801 0.16759439]\n",
      " [0.25561476 0.4732645  0.08455312 0.18656761]\n",
      " [0.6485003  0.27773712 0.02324923 0.05051335]\n",
      " [0.02475022 0.00590639 0.96026553 0.00907787]\n",
      " [0.65561682 0.26521923 0.05412106 0.0250429 ]\n",
      " [0.69344959 0.10614506 0.11007008 0.09033527]\n",
      " [0.56730083 0.41605635 0.00910502 0.0075378 ]\n",
      " [0.01138698 0.93654154 0.04852379 0.00354768]\n",
      " [0.0352315  0.94933812 0.00453091 0.01089947]\n",
      " [0.63568437 0.257063   0.09477414 0.01247849]\n",
      " [0.1736502  0.16858815 0.1726968  0.48506484]\n",
      " [0.53789704 0.33786657 0.099792   0.02444439]\n",
      " [0.07212753 0.15967403 0.02577189 0.74242654]\n",
      " [0.41075373 0.10988847 0.21465611 0.26470169]\n",
      " [0.90088089 0.04525952 0.0291185  0.02474109]\n",
      " [0.59301889 0.28535598 0.09843147 0.02319366]\n",
      " [0.45270889 0.28327657 0.21820514 0.04580939]\n",
      " [0.80427273 0.07240955 0.06487844 0.05843928]\n",
      " [0.67606426 0.13744512 0.16689744 0.01959318]\n",
      " [0.65698298 0.12384343 0.03789694 0.18127666]\n",
      " [0.52588663 0.29070303 0.06346562 0.11994472]\n",
      " [0.72810198 0.04752997 0.13241416 0.09195388]\n",
      " [0.58060332 0.31240562 0.09572967 0.01126139]\n",
      " [0.72496783 0.12726042 0.04178142 0.10599032]\n",
      " [0.90616781 0.02905    0.04031747 0.02446473]\n",
      " [0.60137536 0.27720642 0.10465895 0.01675928]\n",
      " [0.60433906 0.15802107 0.18298922 0.05465064]\n",
      " [0.77440783 0.04557212 0.05087238 0.12914766]\n",
      " [0.66907211 0.09817158 0.15099093 0.08176538]\n",
      " [0.80236233 0.03956729 0.12431559 0.03375479]\n",
      " [0.70980382 0.15362358 0.12187665 0.01469595]\n",
      " [0.00705327 0.00712944 0.05436389 0.9314534 ]\n",
      " [0.6746603  0.05411679 0.15829024 0.11293267]\n",
      " [0.84700549 0.07477927 0.03928533 0.03892991]\n",
      " [0.03631558 0.95724883 0.00304632 0.00338927]\n",
      " [0.26645421 0.32754878 0.12918451 0.2768125 ]\n",
      " [0.62153102 0.19497223 0.04920363 0.13429312]\n",
      " [0.73815598 0.04574603 0.13873053 0.07736746]\n",
      " [0.78118059 0.08734701 0.0887702  0.0427022 ]\n",
      " [0.8270698  0.02945105 0.12392336 0.01955579]\n",
      " [0.02257418 0.94837433 0.01351973 0.01553176]\n",
      " [0.82242297 0.12912132 0.02251234 0.02594337]\n",
      " [0.65684662 0.20810944 0.11485658 0.02018736]\n",
      " [0.69103011 0.27877485 0.01534962 0.01484543]\n",
      " [0.28638295 0.64796165 0.03260826 0.03304714]\n",
      " [0.70985773 0.24283592 0.03100644 0.01629991]\n",
      " [0.61813845 0.25811896 0.07302037 0.05072223]\n",
      " [0.59573888 0.13859137 0.23790247 0.02776728]\n",
      " [0.88554943 0.03772444 0.05353802 0.02318811]\n",
      " [0.70103586 0.16644037 0.01732989 0.11519387]\n",
      " [0.01190793 0.9431279  0.02527282 0.01969135]\n",
      " [0.84483515 0.10096546 0.02975623 0.02444316]\n",
      " [0.58492671 0.11271099 0.09693612 0.20542619]\n",
      " [0.61296506 0.24199795 0.014837   0.13019999]\n",
      " [0.02701804 0.00865489 0.01611976 0.94820731]\n",
      " [0.800346   0.04163109 0.13838015 0.01964276]\n",
      " [0.91864495 0.0309021  0.01895769 0.03149526]\n",
      " [0.66031132 0.20537149 0.11171486 0.02260234]\n",
      " [0.71284192 0.02803741 0.02860727 0.2305134 ]\n",
      " [0.67831527 0.20472646 0.01558779 0.10137048]\n",
      " [0.83712653 0.03010066 0.09248928 0.04028353]\n",
      " [0.84385568 0.03157454 0.09176281 0.03280696]\n",
      " [0.01542175 0.97557863 0.0036664  0.00533322]\n",
      " [0.78639121 0.15096777 0.03253309 0.03010793]\n",
      " [0.76052051 0.06154251 0.14511893 0.03281805]\n",
      " [0.73043764 0.21087236 0.04148098 0.01720902]\n",
      " [0.63356884 0.1870388  0.13915319 0.04023917]\n",
      " [0.62844124 0.13999415 0.1986195  0.03294511]\n",
      " [0.60195714 0.16981502 0.12538964 0.1028382 ]\n",
      " [0.00745304 0.05252063 0.01521764 0.92480868]\n",
      " [0.50812032 0.37380483 0.09797712 0.02009772]\n",
      " [0.08163958 0.55776085 0.33532017 0.0252794 ]\n",
      " [0.72687519 0.09854261 0.12665906 0.04792314]\n",
      " [0.69575361 0.04342345 0.10108562 0.15973732]\n",
      " [0.6994812  0.10143032 0.07157818 0.1275103 ]\n",
      " [0.68473092 0.19294131 0.10730985 0.01501791]\n",
      " [0.60865117 0.26101508 0.1149303  0.01540346]\n",
      " [0.44655782 0.46063487 0.08293783 0.00986947]\n",
      " [0.65531073 0.3026183  0.0224423  0.01962867]\n",
      " [0.85520246 0.06336038 0.04350695 0.03793021]\n",
      " [0.78480417 0.16970435 0.0231765  0.02231497]\n",
      " [0.90291626 0.03133572 0.03469934 0.03104869]\n",
      " [0.46913787 0.46479501 0.04770707 0.01836006]\n",
      " [0.75326872 0.12282351 0.08266538 0.04124239]\n",
      " [0.59518072 0.2207509  0.13720279 0.04686559]\n",
      " [0.58921042 0.25460906 0.10997132 0.0462092 ]\n",
      " [0.82742187 0.06088112 0.06886257 0.04283444]\n",
      " [0.66875057 0.02894829 0.12051131 0.18178983]\n",
      " [0.79970724 0.1439584  0.02515514 0.03117922]\n",
      " [0.72812964 0.04493473 0.03636378 0.19057185]\n",
      " [0.84706201 0.09052427 0.03394695 0.02846677]\n",
      " [0.10632669 0.79635732 0.05460207 0.04271392]\n",
      " [0.80732331 0.04263701 0.12743018 0.0226095 ]\n",
      " [0.71033042 0.05709817 0.07767668 0.15489473]\n",
      " [0.44947482 0.51443891 0.02260939 0.01347688]\n",
      " [0.66453554 0.16642718 0.14551875 0.02351853]\n",
      " [0.90888216 0.02889654 0.03798412 0.02423718]\n",
      " [0.00363779 0.05085506 0.00681542 0.93869172]\n",
      " [0.60112852 0.13312132 0.05537105 0.21037911]\n",
      " [0.69604934 0.26120111 0.01990537 0.02284418]\n",
      " [0.0308317  0.03973502 0.91746643 0.01196685]\n",
      " [0.64441288 0.0462987  0.05535345 0.25393496]\n",
      " [0.60621723 0.34412889 0.03190554 0.01774834]\n",
      " [0.72896297 0.0714059  0.15035851 0.04927261]\n",
      " [0.90455783 0.02828436 0.02371197 0.04344584]\n",
      " [0.79300991 0.04014997 0.11704211 0.04979801]\n",
      " [0.09336738 0.84045384 0.05701013 0.00916866]\n",
      " [0.72781894 0.13940918 0.04675325 0.08601863]\n",
      " [0.52650382 0.3185189  0.11311139 0.04186588]\n",
      " [0.65118098 0.03889641 0.26429772 0.04562489]\n",
      " [0.6570491  0.07414681 0.04120658 0.22759751]\n",
      " [0.7546491  0.13590221 0.0670165  0.04243219]\n",
      " [0.00346929 0.00560466 0.0075096  0.98341646]\n",
      " [0.3889307  0.22024431 0.18991532 0.20090967]\n",
      " [0.63171765 0.28984267 0.04013303 0.03830665]\n",
      " [0.72902833 0.11310994 0.11320693 0.04465481]\n",
      " [0.01141571 0.93365737 0.00512041 0.04980651]\n",
      " [0.75580312 0.0499869  0.15204282 0.04216716]\n",
      " [0.8722676  0.04939111 0.03248466 0.04585663]\n",
      " [0.8523984  0.08346586 0.03759031 0.02654543]\n",
      " [0.73464912 0.02998939 0.04127449 0.194087  ]\n",
      " [0.01745544 0.93411983 0.01367186 0.03475287]\n",
      " [0.00524149 0.07629032 0.05356391 0.86490429]\n",
      " [0.58056327 0.12887021 0.10721671 0.18334981]\n",
      " [0.83668627 0.07931711 0.02376769 0.06022893]\n",
      " [0.77919944 0.14848604 0.04605972 0.0262548 ]\n",
      " [0.67118419 0.16710034 0.1146743  0.04704117]\n",
      " [0.89319915 0.03819957 0.024196   0.04440529]\n",
      " [0.67623388 0.19068376 0.02284968 0.11023269]\n",
      " [0.68637659 0.099024   0.17938487 0.03521454]\n",
      " [0.01203753 0.98143682 0.00318488 0.00334077]\n",
      " [0.6805497  0.27710832 0.01982213 0.02251985]\n",
      " [0.69043175 0.20912065 0.03655602 0.06389158]\n",
      " [0.87305893 0.07155395 0.03006086 0.02532626]\n",
      " [0.70396339 0.21740969 0.01959866 0.05902826]\n",
      " [0.75275223 0.16720542 0.05090171 0.02914064]\n",
      " [0.52674139 0.43255462 0.01729666 0.02340733]\n",
      " [0.732069   0.0371299  0.13124147 0.09955962]\n",
      " [0.58055041 0.34617387 0.0161287  0.05714702]\n",
      " [0.06694424 0.05499272 0.81074804 0.06731499]\n",
      " [0.77798196 0.07643528 0.12338248 0.02220028]\n",
      " [0.68688432 0.27600634 0.02039219 0.01671715]\n",
      " [0.0102616  0.9070255  0.07735638 0.00535652]\n",
      " [0.76643226 0.17213823 0.02556384 0.03586567]\n",
      " [0.05596207 0.01253635 0.0284195  0.90308208]\n",
      " [0.87194432 0.0543196  0.03920407 0.03453202]]\n",
      "TYPE: (200, 4) <class 'numpy.ndarray'>\n",
      "ret <class 'numpy.ndarray'>\n",
      "[[0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.23939487 0.76060513]\n",
      " [1.         0.        ]\n",
      " [0.66116159 0.33883841]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.61989514 0.38010486]\n",
      " [0.70154113 0.29845887]\n",
      " [0.67498496 0.32501504]\n",
      " [0.67974516 0.32025484]\n",
      " [0.60997036 0.39002964]\n",
      " [0.54718602 0.45281398]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.42182102 0.57817898]\n",
      " [0.67643141 0.32356859]\n",
      " [0.61999488 0.38000512]\n",
      " [0.60484306 0.39515694]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.66150174 0.33849826]\n",
      " [0.         1.        ]\n",
      " [0.70879186 0.29120814]\n",
      " [0.58930797 0.41069203]\n",
      " [0.65326156 0.34673844]\n",
      " [0.65662235 0.34337765]\n",
      " [1.         0.        ]\n",
      " [0.66339274 0.33660726]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.69637893 0.30362107]\n",
      " [1.         0.        ]\n",
      " [0.64153721 0.35846279]\n",
      " [0.67338312 0.32661688]\n",
      " [0.62479614 0.37520386]\n",
      " [0.62395687 0.37604313]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.59724288 0.40275712]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.32129185 0.67870815]\n",
      " [0.62924645 0.37075355]\n",
      " [1.         0.        ]\n",
      " [0.68202845 0.31797155]\n",
      " [0.54501728 0.45498272]\n",
      " [0.6792936  0.3207064 ]\n",
      " [0.61990519 0.38009481]\n",
      " [1.         0.        ]\n",
      " [0.25561476 0.74438524]\n",
      " [0.6485003  0.3514997 ]\n",
      " [0.         1.        ]\n",
      " [0.65561682 0.34438318]\n",
      " [0.69344959 0.30655041]\n",
      " [0.56730083 0.43269917]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.63568437 0.36431563]\n",
      " [0.1736502  0.8263498 ]\n",
      " [0.53789704 0.46210296]\n",
      " [0.         1.        ]\n",
      " [0.41075373 0.58924627]\n",
      " [1.         0.        ]\n",
      " [0.59301889 0.40698111]\n",
      " [0.45270889 0.54729111]\n",
      " [1.         0.        ]\n",
      " [0.67606426 0.32393574]\n",
      " [0.65698298 0.34301702]\n",
      " [0.52588663 0.47411337]\n",
      " [1.         0.        ]\n",
      " [0.58060332 0.41939668]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.60137536 0.39862464]\n",
      " [0.60433906 0.39566094]\n",
      " [1.         0.        ]\n",
      " [0.66907211 0.33092789]\n",
      " [1.         0.        ]\n",
      " [0.70980382 0.29019618]\n",
      " [0.         1.        ]\n",
      " [0.6746603  0.3253397 ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.26645421 0.73354579]\n",
      " [0.62153102 0.37846898]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.65684662 0.34315338]\n",
      " [0.69103011 0.30896989]\n",
      " [0.28638295 0.71361705]\n",
      " [0.70985773 0.29014227]\n",
      " [0.61813845 0.38186155]\n",
      " [0.59573888 0.40426112]\n",
      " [1.         0.        ]\n",
      " [0.70103586 0.29896414]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.58492671 0.41507329]\n",
      " [0.61296506 0.38703494]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.66031132 0.33968868]\n",
      " [1.         0.        ]\n",
      " [0.67831527 0.32168473]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.63356884 0.36643116]\n",
      " [0.62844124 0.37155876]\n",
      " [0.60195714 0.39804286]\n",
      " [0.         1.        ]\n",
      " [0.50812032 0.49187968]\n",
      " [0.08163958 0.91836042]\n",
      " [1.         0.        ]\n",
      " [0.69575361 0.30424639]\n",
      " [0.6994812  0.3005188 ]\n",
      " [0.68473092 0.31526908]\n",
      " [0.60865117 0.39134883]\n",
      " [0.44655782 0.55344218]\n",
      " [0.65531073 0.34468927]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.46913787 0.53086213]\n",
      " [1.         0.        ]\n",
      " [0.59518072 0.40481928]\n",
      " [0.58921042 0.41078958]\n",
      " [1.         0.        ]\n",
      " [0.66875057 0.33124943]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.10632669 0.89367331]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.44947482 0.55052518]\n",
      " [0.66453554 0.33546446]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.60112852 0.39887148]\n",
      " [0.69604934 0.30395066]\n",
      " [0.         1.        ]\n",
      " [0.64441288 0.35558712]\n",
      " [0.60621723 0.39378277]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.09336738 0.90663262]\n",
      " [1.         0.        ]\n",
      " [0.52650382 0.47349618]\n",
      " [0.65118098 0.34881902]\n",
      " [0.6570491  0.3429509 ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.3889307  0.6110693 ]\n",
      " [0.63171765 0.36828235]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.58056327 0.41943673]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.67118419 0.32881581]\n",
      " [1.         0.        ]\n",
      " [0.67623388 0.32376612]\n",
      " [0.68637659 0.31362341]\n",
      " [0.         1.        ]\n",
      " [0.6805497  0.3194503 ]\n",
      " [0.69043175 0.30956825]\n",
      " [1.         0.        ]\n",
      " [0.70396339 0.29603661]\n",
      " [1.         0.        ]\n",
      " [0.52674139 0.47325861]\n",
      " [1.         0.        ]\n",
      " [0.58055041 0.41944959]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.68688432 0.31311568]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]]\n",
      "0      1.0\n",
      "1      0.0\n",
      "2      0.0\n",
      "3      0.0\n",
      "4      0.0\n",
      "5      1.0\n",
      "6      0.0\n",
      "7      0.0\n",
      "8      0.0\n",
      "9      0.0\n",
      "10     0.0\n",
      "11     0.0\n",
      "12     0.0\n",
      "13     1.0\n",
      "14     0.0\n",
      "15     0.0\n",
      "16     0.0\n",
      "17     0.0\n",
      "18     0.0\n",
      "19     0.0\n",
      "20     0.0\n",
      "21     0.0\n",
      "22     1.0\n",
      "23     0.0\n",
      "24     1.0\n",
      "25     0.0\n",
      "26     0.0\n",
      "27     0.0\n",
      "28     0.0\n",
      "29     0.0\n",
      "30     0.0\n",
      "31     1.0\n",
      "32     0.0\n",
      "33     0.0\n",
      "34     0.0\n",
      "35     1.0\n",
      "36     0.0\n",
      "37     0.0\n",
      "38     0.0\n",
      "39     0.0\n",
      "40     0.0\n",
      "41     0.0\n",
      "42     1.0\n",
      "43     0.0\n",
      "44     0.0\n",
      "45     0.0\n",
      "46     0.0\n",
      "47     1.0\n",
      "48     0.0\n",
      "49     0.0\n",
      "50     0.0\n",
      "51     0.0\n",
      "52     0.0\n",
      "53     0.0\n",
      "54     0.0\n",
      "55     0.0\n",
      "56     0.0\n",
      "57     1.0\n",
      "58     0.0\n",
      "59     0.0\n",
      "60     0.0\n",
      "61     1.0\n",
      "62     1.0\n",
      "63     0.0\n",
      "64     1.0\n",
      "65     0.0\n",
      "66     1.0\n",
      "67     0.0\n",
      "68     0.0\n",
      "69     0.0\n",
      "70     0.0\n",
      "71     0.0\n",
      "72     0.0\n",
      "73     0.0\n",
      "74     0.0\n",
      "75     0.0\n",
      "76     0.0\n",
      "77     0.0\n",
      "78     0.0\n",
      "79     0.0\n",
      "80     0.0\n",
      "81     0.0\n",
      "82     0.0\n",
      "83     0.0\n",
      "84     0.0\n",
      "85     1.0\n",
      "86     0.0\n",
      "87     0.0\n",
      "88     1.0\n",
      "89     0.0\n",
      "90     0.0\n",
      "91     0.0\n",
      "92     0.0\n",
      "93     0.0\n",
      "94     1.0\n",
      "95     0.0\n",
      "96     0.0\n",
      "97     0.0\n",
      "98     1.0\n",
      "99     0.0\n",
      "100    0.0\n",
      "101    0.0\n",
      "102    0.0\n",
      "103    0.0\n",
      "104    1.0\n",
      "105    0.0\n",
      "106    0.0\n",
      "107    0.0\n",
      "108    1.0\n",
      "109    0.0\n",
      "110    0.0\n",
      "111    0.0\n",
      "112    0.0\n",
      "113    0.0\n",
      "114    0.0\n",
      "115    0.0\n",
      "116    1.0\n",
      "117    0.0\n",
      "118    0.0\n",
      "119    0.0\n",
      "120    0.0\n",
      "121    0.0\n",
      "122    0.0\n",
      "123    1.0\n",
      "124    0.0\n",
      "125    1.0\n",
      "126    0.0\n",
      "127    0.0\n",
      "128    0.0\n",
      "129    0.0\n",
      "130    0.0\n",
      "131    0.0\n",
      "132    0.0\n",
      "133    0.0\n",
      "134    0.0\n",
      "135    0.0\n",
      "136    0.0\n",
      "137    0.0\n",
      "138    0.0\n",
      "139    0.0\n",
      "140    0.0\n",
      "141    0.0\n",
      "142    0.0\n",
      "143    0.0\n",
      "144    0.0\n",
      "145    1.0\n",
      "146    0.0\n",
      "147    0.0\n",
      "148    0.0\n",
      "149    0.0\n",
      "150    0.0\n",
      "151    1.0\n",
      "152    0.0\n",
      "153    0.0\n",
      "154    1.0\n",
      "155    0.0\n",
      "156    0.0\n",
      "157    0.0\n",
      "158    0.0\n",
      "159    0.0\n",
      "160    1.0\n",
      "161    0.0\n",
      "162    0.0\n",
      "163    0.0\n",
      "164    0.0\n",
      "165    0.0\n",
      "166    1.0\n",
      "167    0.0\n",
      "168    0.0\n",
      "169    0.0\n",
      "170    1.0\n",
      "171    0.0\n",
      "172    0.0\n",
      "173    0.0\n",
      "174    0.0\n",
      "175    1.0\n",
      "176    1.0\n",
      "177    0.0\n",
      "178    0.0\n",
      "179    0.0\n",
      "180    0.0\n",
      "181    0.0\n",
      "182    0.0\n",
      "183    0.0\n",
      "184    1.0\n",
      "185    0.0\n",
      "186    0.0\n",
      "187    0.0\n",
      "188    0.0\n",
      "189    0.0\n",
      "190    0.0\n",
      "191    0.0\n",
      "192    0.0\n",
      "193    1.0\n",
      "194    0.0\n",
      "195    0.0\n",
      "196    1.0\n",
      "197    0.0\n",
      "198    1.0\n",
      "199    0.0\n",
      "Name: Class, dtype: float64\n",
      "0.09473943881352236\n"
     ]
    }
   ],
   "source": [
    "ensemble = models[0]\n",
    "\n",
    "y_pred = ensemble.predict_proba(test)\n",
    "print(y_pred)\n",
    "shape = test.shape[0]\n",
    "y_p = calibrate_prob(y_pred, shape, best_thres[0], best_thres[1])\n",
    "loss = balanced_log_loss(y_val, y_p)\n",
    "\n",
    "\n",
    "\n",
    "print(y_p)\n",
    "print(y_val)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ffffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ensembles = list()\n",
    "for ensemble in models:\n",
    "    y_pred = ensemble.predict_proba(test)\n",
    "    print(y_pred)\n",
    "    shape = test.shape[0]\n",
    "    y_p = calibrate_prob(y_pred, shape, best_thres[0], best_thres[1])\n",
    "    loss = balanced_log_loss(y_val, y_p)\n",
    "    loss_ensembles.append(loss)\n",
    "    final_loss = np.mean(loss_ensembles)\n",
    "\n",
    "print('lost_ensembles', loss_ensembles)\n",
    "print('LOSS', final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96bd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = test.shape[0]\n",
    "\n",
    "y_p = calibrate_prob(y_pred, shape, 0.90)\n",
    "y_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf18db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = a.Class\n",
    "loss = balanced_log_loss(y_val, y_p)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec3a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "submission = pd.DataFrame(Id, columns=['Id'])\n",
    "submission[\"class_0\"] = y_p[:, 0]\n",
    "submission[\"class_1\"] = y_p[:, 1]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ecbdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70353514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
